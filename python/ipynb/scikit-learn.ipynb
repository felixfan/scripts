{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Logistic Regression</a></div><div class=\"lev1 toc-item\"><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Naive Bayes</a></div><div class=\"lev1 toc-item\"><a href=\"#k-Nearest-Neighbor\" data-toc-modified-id=\"k-Nearest-Neighbor-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>k-Nearest Neighbor</a></div><div class=\"lev1 toc-item\"><a href=\"#Classification-and-Regression-Trees\" data-toc-modified-id=\"Classification-and-Regression-Trees-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Classification and Regression Trees</a></div><div class=\"lev1 toc-item\"><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Support Vector Machines</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression fits a logistic model to data and makes predictions about the probability of an event (between 0 and 1).\n",
    "\n",
    "This recipe shows the fitting of a logistic regression model to the iris dataset. Because this is a mutli-class classification problem and logistic regression makes predictions between 0 and 1, a one-vs-all scheme is used (one model per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       0.98      0.90      0.94        50\n",
      "          2       0.91      0.98      0.94        50\n",
      "\n",
      "avg / total       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 45  5]\n",
      " [ 0  1 49]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes uses Bayes Theorem to model the conditional relationship of each attribute to the class variable.\n",
    "\n",
    "This recipe shows the fitting of an Naive Bayes model to the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       0.94      0.94      0.94        50\n",
      "          2       0.94      0.94      0.94        50\n",
      "\n",
      "avg / total       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a Naive Bayes model to the data\n",
    "model = GaussianNB()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbor (kNN) method makes predictions by locating similar cases to a given data instance (using a similarity function) and returning the average or majority of the most similar data instances. The kNN algorithm can be used for classification or regression.\n",
    "\n",
    "This recipe shows use of the kNN model to make predictions for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       0.96      0.94      0.95        50\n",
      "          2       0.94      0.96      0.95        50\n",
      "\n",
      "avg / total       0.97      0.97      0.97       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  2 48]]\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbor\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# load iris the datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Regression Trees (CART) are constructed from a dataset by making splits that best separate the data for the classes or predictions being made. The CART algorithm can be used for classification or regression.\n",
    "\n",
    "This recipe shows use of the CART model to make predictions for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       1.00      1.00      1.00        50\n",
      "          2       1.00      1.00      1.00        50\n",
      "\n",
      "avg / total       1.00      1.00      1.00       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a method that uses points in a transformed problem space that best separate classes into two groups. Classification for multiple classes is supported by a one-vs-all method. SVM also supports regression by modeling the function with a minimum amount of allowable error.\n",
    "\n",
    "This recipe shows use of the SVM model to make predictions for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       1.00      0.96      0.98        50\n",
      "          2       0.96      1.00      0.98        50\n",
      "\n",
      "avg / total       0.99      0.99      0.99       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 48  2]\n",
      " [ 0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "6",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
