---
title: "Ensemble Learning"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Decision Trees in R

## 1.1 Loading required libraries rpart, party and partykit

```{r comment='',warning=FALSE,message=FALSE}
library(rpart)
library(party)
library(partykit)

set.seed(-999)
```

## 1.2 Accessing credit scores

```{r comment='',warning=FALSE,message=FALSE}
library(caret)
data(GermanCredit)
head(GermanCredit[,1:5])
```

## 1.3 Split data into index subset for training (80 %) and testing (20 %) instances

```{r comment='',warning=FALSE,message=FALSE}
inTrain <- runif(nrow(GermanCredit)) < 0.8
training <- GermanCredit[inTrain,]
testing <- GermanCredit[-inTrain,]
```

## 1.4 Building a decision tree with rpart

```{r comment='',warning=FALSE,message=FALSE}
dt <- rpart(Class ~ Duration + Amount + Age, method="class", data=training)
```

## 1.5 Plot decision tree using plot(dt) and text(dt)

```{r comment='',warning=FALSE,message=FALSE}
plot(dt)
text(dt)
```

## 1.6 Drawing Decision Trees Nicely

```{r comment='',warning=FALSE,message=FALSE}
plot(as.party(dt))
```

## 1.7 Complexity Parameter

```{r comment='',warning=FALSE,message=FALSE}
printcp(dt)
```

- Rows show results for trees with different numbers of nodes    
- Cross-validation error in column xerror    
- Complexity parameter in column CP, similar to number of nodes    

## 1.8 Pruning Decision Trees

- Reduce tree size by removing nodes with little predictive power     
- Aim: Minimize cross-validation error in column xerror    

```{r comment='',warning=FALSE,message=FALSE}
m <- which.min(dt$cptable[, "xerror"])
```

- Optimal number of splits    

```{r comment='',warning=FALSE,message=FALSE}
dt$cptable[m, "nsplit"]
```

- Choose corresponding complexity parameter    

```{r comment='',warning=FALSE,message=FALSE}
dt$cptable[m, "CP"]
```

```{r comment='',warning=FALSE,message=FALSE}
p <- prune(dt, cp = dt$cptable[which.min(dt$cptable[, "xerror"]), "CP"])
plot(as.party(p))
```

## 1.9 Prediction with Decision Trees

```{r comment='',warning=FALSE,message=FALSE}
pred <- predict(p, testing, type="class")
pred[1:5]
```

## 1.10 Confusion matrix

- horizontal: true class;    
- vertical: predicted class    

```{r comment='',warning=FALSE,message=FALSE}
table(pred=pred, true=testing$Class)
```

# 2. Concepts of Ensemble Learning

- Combine predictions of multiple learning algorithms → ensemble    
- Often leads to a better predictive performance than a single learner    
- Well-suited when small differences in the training data produce very different classifiers (e. g. decision trees)    
- Drawbacks: increases computation time, reduces interpretability      

**Reasoning**        

- Classifiers $C_{1}$,...,$C_{K}$ which are independent, i. e. cor($C_{i}$, $C_{j}$) = 0       
- Each has an error probability of $P_{i}$ < 0.5 on the training data     
- Then an ensemble of classifiers should have an error probability lower than each individual $P_{i}$   

**Example**   

- Given K classifiers, each with the same error probability $P_{ε}$ = 0.3
- Probability that exactly L classifiers make an error is    
$$\binom{K}{L}P_{ε}^L(1-P_{ε})^{K-L}$$
- Only if Pε > 0.5, the error rate of the ensemble increases    

## 2.1 Various methods exist for ensemble learning

### 2.1.1 Constructing ensembles: methods for obtaining a set of classifiers

- Bagging (also named Bootstrap Aggregation)    
- Random Forest   
- Cross-validation      

### 2.1.2 Combining classifiers: methods for combining different classifiers

- Stacking    
- Bayesian Model Averaging   
- Boosting   
- AdaBoost   

# 3. Bagging (Bootstrap Aggregation) 

**Idea**    

- Reuse the same training algorithm several times on different subsets of the training data   

![](fig/Ensemble-bagging.png)   

**Algorithm**   

- Given training set D of size N    
- Bagging generates new training sets $D_{i}$ of size M by sampling with replacement from D    
- Some observations may be repeated in each $D_{i}$    
- If M = N, then on average 63.2% (Breiman, 1996) of the original training dataset D is represented, the rest are duplicates     
- Afterwards train classifier on each $D_{i}$ separately   

# 4. Random Forests

- Random Forests are an ensemble learning method for classification and regression      
- It combines multiple individual decision trees by means of bagging      
- Overcomes the problem of overfitting decision trees     

**Algorithm**   

- Create many decision trees by bagging     
- Inject randomness into decision trees    
  -- Tree grows to maximum size and is left unpruned    
  -- Each split is based on randomly selected subset of attributes    
- Ensemble trees (i. e. the random forest) vote on categories by majority   

**Advantages**   

- Simple algorithm that learns non-linearity    
- Good performance in practice      
- Fast training algorithm    
- Resistant to overfitting    

**Limitations**   

- High memory consumption during tree construction    
- Little performance gain from large training data   

## 4.1 Random Forests in R

```{r comment='',warning=FALSE,message=FALSE}
library(randomForest)
rf <- randomForest(Class ~ ., data=training, ntree=100)
```

Options to control behavior:    

- `ntree` controls the number of trees (default: 500)    
- `mtry` gives number of variables to choose from at each node    
- `na.action` specifies how to handle missing values     
- `importance=TRUE` calculates variable importance metric    

```{r comment='',warning=FALSE,message=FALSE}
plot(rf)
```

- Dotted lines represent corresponding error of classes and solid black line represents overall error   

```{r comment='',warning=FALSE,message=FALSE}
rf$confusion
```

```{r comment='',warning=FALSE,message=FALSE}
pred <- predict(rf, newdata=testing)
table(pred=pred, true=testing$Class)
```

## 4.2 Variable Importance in R

```{r comment='',warning=FALSE,message=FALSE}
rf2 <- randomForest(Class ~ ., 
                    data=GermanCredit, #with full dataset
                    ntree=100,
                    importance=TRUE)
```

```{r comment='',warning=FALSE,message=FALSE}
varImpPlot(rf2, type=1, n.var=5)
```

- `type` choose the importance metric (= 1 is the mean decrease in accuracy if the variable would be randomly permuted)    
- `n.var` denotes number of variables    

# 5. Boosting

- Combine multiple classifiers to improve classification accuracy    
- Works together with many different types of classifiers    
- None of the classifier needs extremely good, only better than chance    
- Idea: train classifiers on a subset of the training data that is most informative given the current classifiers   
- Yields sequential classifier selection    

## 5.1 Boosting in R

```{r comment='',warning=FALSE,message=FALSE}
library(mboost)
```

### 5.1.1 Fit a generalized linear model via glmboost(...)   

```{r comment='',warning=FALSE,message=FALSE}
m.boost <- glmboost(Class ~ Amount + Duration + Personal.Female.Single,
                    family=Binomial(), # needed for classification
                    data=GermanCredit)
```

```{r comment='',warning=FALSE,message=FALSE}
coef(m.boost)
```

```{r comment='',warning=FALSE,message=FALSE}
plot(m.boost, ylim=range(coef(m.boost, which=c("Amount", "Duration"))))
```

```{r comment='',warning=FALSE,message=FALSE}
cv.boost <- cvrisk(m.boost)
mstop(cv.boost)               # optimal no. of iterations to prevent overfitting
```

```{r comment='',warning=FALSE,message=FALSE}
plot(cv.boost, main="Cross-validated estimates of empirical risk")
```

### 5.1.2 fit generalized additive model via component-wise boosting

```{r comment='',warning=FALSE,message=FALSE}
m.boost <- gamboost(Class ~ Amount + Duration,
                    family=Binomial(), # needed for classification
                    data=GermanCredit)
m.boost
```

# 6. AdaBoost

- Instead of resampling, reweight misclassified training examples   

**Benefits**   

- Simple combination of multiple classifiers    
- Easy implementation    
- Different types of classifiers can be used     
- Commonly in used across many domains   

**Limitations**   

- Sensitive to misclassified points in training data   

## 6.1 AdaBoost in R

```{r comment='',warning=FALSE,message=FALSE}
library(ada)
```

```{r comment='',warning=FALSE,message=FALSE}
m.ada <- ada(Class ~ ., data=training, iter=50)
```

```{r comment='',warning=FALSE,message=FALSE}
m.ada.test <- addtest(m.ada, test.x=testing, test.y=testing$Class)
```

```{r comment='',warning=FALSE,message=FALSE}
m.ada.test
```

```{r comment='',warning=FALSE,message=FALSE}
plot(m.ada.test, test=TRUE)
```

```{r comment='',warning=FALSE,message=FALSE}
varplot(m.ada.test, max.var.show=5) # first 5 variables
```
