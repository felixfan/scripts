---
title: "Ensemble Learning"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Decision Trees in R

## 1.1 Loading required libraries rpart, party and partykit

```{r comment='',warning=FALSE,message=FALSE}
library(rpart)
library(party)
library(partykit)

set.seed(-999)
```

## 1.2 Accessing credit scores

```{r comment='',warning=FALSE,message=FALSE}
library(caret)
data(GermanCredit)
head(GermanCredit[,1:5])
```

## 1.3 Split data into index subset for training (80 %) and testing (20 %) instances

```{r comment='',warning=FALSE,message=FALSE}
inTrain <- runif(nrow(GermanCredit)) < 0.8
training <- GermanCredit[inTrain,]
testing <- GermanCredit[-inTrain,]
```

## 1.4 Building a decision tree with rpart

```{r comment='',warning=FALSE,message=FALSE}
dt <- rpart(Class ~ Duration + Amount + Age, method="class", data=training)
```

## 1.5 Plot decision tree using plot(dt) and text(dt)

```{r comment='',warning=FALSE,message=FALSE}
plot(dt)
text(dt)
```

## 1.6 Drawing Decision Trees Nicely

```{r comment='',warning=FALSE,message=FALSE}
plot(as.party(dt))
```

## 1.7 Complexity Parameter

```{r comment='',warning=FALSE,message=FALSE}
printcp(dt)
```

- Rows show results for trees with different numbers of nodes    
- Cross-validation error in column xerror    
- Complexity parameter in column CP, similar to number of nodes    

## 1.8 Pruning Decision Trees

- Reduce tree size by removing nodes with little predictive power     
- Aim: Minimize cross-validation error in column xerror    

```{r comment='',warning=FALSE,message=FALSE}
m <- which.min(dt$cptable[, "xerror"])
```

- Optimal number of splits    

```{r comment='',warning=FALSE,message=FALSE}
dt$cptable[m, "nsplit"]
```

- Choose corresponding complexity parameter    

```{r comment='',warning=FALSE,message=FALSE}
dt$cptable[m, "CP"]
```

```{r comment='',warning=FALSE,message=FALSE}
p <- prune(dt, cp = dt$cptable[which.min(dt$cptable[, "xerror"]), "CP"])
plot(as.party(p))
```

## 1.9 Prediction with Decision Trees

```{r comment='',warning=FALSE,message=FALSE}
pred <- predict(p, testing, type="class")
pred[1:5]
```

## 1.10 Confusion matrix

- horizontal: true class;    
- vertical: predicted class    

```{r comment='',warning=FALSE,message=FALSE}
table(pred=pred, true=testing$Class)
```

# 2. Concepts of Ensemble Learning

- Combine predictions of multiple learning algorithms → ensemble    
- Often leads to a better predictive performance than a single learner    
- Well-suited when small differences in the training data produce very different classifiers (e. g. decision trees)    
- Drawbacks: increases computation time, reduces interpretability      

**Reasoning**        

- Classifiers $C_{1}$,...,$C_{K}$ which are independent, i. e. cor($C_{i}$, $C_{j}$) = 0       
- Each has an error probability of $P_{i}$ < 0.5 on the training data     
- Then an ensemble of classifiers should have an error probability lower than each individual $P_{i}$   

**Example**   

- Given K classifiers, each with the same error probability $P_{ε}$ = 0.3
- Probability that exactly L classifiers make an error is    
$$\binom{K}{L}P_{ε}^L(1-P_{ε})^{K-L}$$
- Only if Pε > 0.5, the error rate of the ensemble increases    

## 2.1 Various methods exist for ensemble learning

### 2.1.1 Constructing ensembles: methods for obtaining a set of classifiers

- Bagging (also named Bootstrap Aggregation)    
- Random Forest   
- Cross-validation      

### 2.1.2 Combining classifiers: methods for combining different classifiers

- Stacking    
- Bayesian Model Averaging   
- Boosting   
- AdaBoost   

# 3. Bagging (Bootstrap Aggregation) 

**Idea**    

- Reuse the same training algorithm several times on different subsets of the training data   

![](fig/Ensemble-bagging.png)   

**Algorithm**   

- Given training set D of size N    
- Bagging generates new training sets $D_{i}$ of size M by sampling with replacement from D    
- Some observations may be repeated in each $D_{i}$    
- If M = N, then on average 63.2% (Breiman, 1996) of the original training dataset D is represented, the rest are duplicates     
- Afterwards train classifier on each $D_{i}$ separately   

# 4. Random Forests

# 5. Boosting

# 6. AdaBoost



```{r comment='',warning=FALSE,message=FALSE}

```
```{r comment='',warning=FALSE,message=FALSE}

```
```{r comment='',warning=FALSE,message=FALSE}

```
```{r comment='',warning=FALSE,message=FALSE}

```
```{r comment='',warning=FALSE,message=FALSE}

```
```{r comment='',warning=FALSE,message=FALSE}

```





