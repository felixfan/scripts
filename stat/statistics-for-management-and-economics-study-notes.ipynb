{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#1.-What-is-Statistics?\" data-toc-modified-id=\"1.-What-is-Statistics?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. What is Statistics?</a></div><div class=\"lev1 toc-item\"><a href=\"#2.-Graphical-Descriptive-Techniques-I\" data-toc-modified-id=\"2.-Graphical-Descriptive-Techniques-I-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Graphical Descriptive Techniques I</a></div><div class=\"lev2 toc-item\"><a href=\"#2.1-Types-of-Data-and-Information\" data-toc-modified-id=\"2.1-Types-of-Data-and-Information-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>2.1 Types of Data and Information</a></div><div class=\"lev2 toc-item\"><a href=\"#2.2-Describing-a-Set-of-Nominal-Data\" data-toc-modified-id=\"2.2-Describing-a-Set-of-Nominal-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>2.2 Describing a Set of Nominal Data</a></div><div class=\"lev1 toc-item\"><a href=\"#3.-Graphical-Descriptive-Techniques-II\" data-toc-modified-id=\"3.-Graphical-Descriptive-Techniques-II-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Graphical Descriptive Techniques II</a></div><div class=\"lev2 toc-item\"><a href=\"#3.1-Describing-a-Set-of-Interval-Data\" data-toc-modified-id=\"3.1-Describing-a-Set-of-Interval-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>3.1 Describing a Set of Interval Data</a></div><div class=\"lev2 toc-item\"><a href=\"#3.1.1-Histogram\" data-toc-modified-id=\"3.1.1-Histogram-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>3.1.1 Histogram</a></div><div class=\"lev2 toc-item\"><a href=\"#3.1.1.1-Determining-the-Number-of-Class-Intervals\" data-toc-modified-id=\"3.1.1.1-Determining-the-Number-of-Class-Intervals-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>3.1.1.1 Determining the Number of Class Intervals</a></div><div class=\"lev2 toc-item\"><a href=\"#3.1.2-Stem-and-Leaf-Display\" data-toc-modified-id=\"3.1.2-Stem-and-Leaf-Display-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>3.1.2 Stem-and-Leaf Display</a></div><div class=\"lev2 toc-item\"><a href=\"#3.2-Describing-time-series-Data\" data-toc-modified-id=\"3.2-Describing-time-series-Data-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>3.2 Describing time-series Data</a></div><div class=\"lev2 toc-item\"><a href=\"#3.3-Describing-the-Relationship-between-Two-Interval-Data\" data-toc-modified-id=\"3.3-Describing-the-Relationship-between-Two-Interval-Data-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>3.3 Describing the Relationship between Two Interval Data</a></div><div class=\"lev1 toc-item\"><a href=\"#4.-Numerical-Descriptive-Techniques\" data-toc-modified-id=\"4.-Numerical-Descriptive-Techniques-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Numerical Descriptive Techniques</a></div><div class=\"lev2 toc-item\"><a href=\"#4.1-Measure-of-Central-Location\" data-toc-modified-id=\"4.1-Measure-of-Central-Location-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>4.1 Measure of Central Location</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1.1-Arithmetic-Mean\" data-toc-modified-id=\"4.1.1-Arithmetic-Mean-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>4.1.1 Arithmetic Mean</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1.2-Median\" data-toc-modified-id=\"4.1.2-Median-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>4.1.2 Median</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1.3-Mode\" data-toc-modified-id=\"4.1.3-Mode-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>4.1.3 Mode</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1.4-Mean,-Median,-Mode:-Which-Is-Best?\" data-toc-modified-id=\"4.1.4-Mean,-Median,-Mode:-Which-Is-Best?-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>4.1.4 Mean, Median, Mode: Which Is Best?</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1.5-Geometric-mean\" data-toc-modified-id=\"4.1.5-Geometric-mean-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>4.1.5 Geometric mean</a></div><div class=\"lev2 toc-item\"><a href=\"#4.2-Measures-of-Variability\" data-toc-modified-id=\"4.2-Measures-of-Variability-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>4.2 Measures of Variability</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2.1-Range\" data-toc-modified-id=\"4.2.1-Range-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>4.2.1 Range</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2.2-Variance\" data-toc-modified-id=\"4.2.2-Variance-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>4.2.2 Variance</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2.3-Standard-Deviation\" data-toc-modified-id=\"4.2.3-Standard-Deviation-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>4.2.3 Standard Deviation</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2.4-Chebysheff’s-Theorem\" data-toc-modified-id=\"4.2.4-Chebysheff’s-Theorem-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>4.2.4 Chebysheff’s Theorem</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2.5-Coefficient-of-Variation\" data-toc-modified-id=\"4.2.5-Coefficient-of-Variation-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>4.2.5 Coefficient of Variation</a></div><div class=\"lev2 toc-item\"><a href=\"#4.3-Measures-of-Relative-Standing-and-Box-Plots\" data-toc-modified-id=\"4.3-Measures-of-Relative-Standing-and-Box-Plots-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>4.3 Measures of Relative Standing and Box Plots</a></div><div class=\"lev3 toc-item\"><a href=\"#4.3.1-Percentile\" data-toc-modified-id=\"4.3.1-Percentile-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>4.3.1 Percentile</a></div><div class=\"lev3 toc-item\"><a href=\"#4.3.2-Locating-Percentiles\" data-toc-modified-id=\"4.3.2-Locating-Percentiles-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>4.3.2 Locating Percentiles</a></div><div class=\"lev3 toc-item\"><a href=\"#4.3.3-Interquartile-Range\" data-toc-modified-id=\"4.3.3-Interquartile-Range-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>4.3.3 Interquartile Range</a></div><div class=\"lev3 toc-item\"><a href=\"#4.3.4-Box-Plots\" data-toc-modified-id=\"4.3.4-Box-Plots-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>4.3.4 Box Plots</a></div><div class=\"lev2 toc-item\"><a href=\"#4.4-Measures-of-Linear-Relationship\" data-toc-modified-id=\"4.4-Measures-of-Linear-Relationship-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>4.4 Measures of Linear Relationship</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4.1-Covariance\" data-toc-modified-id=\"4.4.1-Covariance-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>4.4.1 Covariance</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4.2-Coefficient-of-Correlation\" data-toc-modified-id=\"4.4.2-Coefficient-of-Correlation-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>4.4.2 Coefficient of Correlation</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4.3-Least-Squares-Method\" data-toc-modified-id=\"4.4.3-Least-Squares-Method-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>4.4.3 Least Squares Method</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4.4-Coefficient-of-Determination\" data-toc-modified-id=\"4.4.4-Coefficient-of-Determination-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>4.4.4 Coefficient of Determination</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4.5-Interpreting-Correlation\" data-toc-modified-id=\"4.4.5-Interpreting-Correlation-4.4.5\"><span class=\"toc-item-num\">4.4.5&nbsp;&nbsp;</span>4.4.5 Interpreting Correlation</a></div><div class=\"lev1 toc-item\"><a href=\"#5.-Data-Collection-And-Sampling\" data-toc-modified-id=\"5.-Data-Collection-And-Sampling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Data Collection And Sampling</a></div><div class=\"lev2 toc-item\"><a href=\"#5.1-Simple-Random-Sample\" data-toc-modified-id=\"5.1-Simple-Random-Sample-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>5.1 Simple Random Sample</a></div><div class=\"lev2 toc-item\"><a href=\"#5.2-Stratified-Random-Sampling\" data-toc-modified-id=\"5.2-Stratified-Random-Sampling-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>5.2 Stratified Random Sampling</a></div><div class=\"lev2 toc-item\"><a href=\"#5.3-Cluster-Sampling\" data-toc-modified-id=\"5.3-Cluster-Sampling-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>5.3 Cluster Sampling</a></div><div class=\"lev2 toc-item\"><a href=\"#5.4-Sampling-Error\" data-toc-modified-id=\"5.4-Sampling-Error-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>5.4 Sampling Error</a></div><div class=\"lev2 toc-item\"><a href=\"#5.5-Nonsampling-Error\" data-toc-modified-id=\"5.5-Nonsampling-Error-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>5.5 Nonsampling Error</a></div><div class=\"lev1 toc-item\"><a href=\"#6-Probability\" data-toc-modified-id=\"6-Probability-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>6 Probability</a></div><div class=\"lev2 toc-item\"><a href=\"#6.1-Intersection\" data-toc-modified-id=\"6.1-Intersection-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>6.1 Intersection</a></div><div class=\"lev2 toc-item\"><a href=\"#6.2-Marginal-Probability\" data-toc-modified-id=\"6.2-Marginal-Probability-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>6.2 Marginal Probability</a></div><div class=\"lev2 toc-item\"><a href=\"#6.3-Conditional-Probability\" data-toc-modified-id=\"6.3-Conditional-Probability-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>6.3 Conditional Probability</a></div><div class=\"lev2 toc-item\"><a href=\"#6.4-Independence\" data-toc-modified-id=\"6.4-Independence-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>6.4 Independence</a></div><div class=\"lev2 toc-item\"><a href=\"#6.5-Union\" data-toc-modified-id=\"6.5-Union-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>6.5 Union</a></div><div class=\"lev2 toc-item\"><a href=\"#6.6-Complement-Rule\" data-toc-modified-id=\"6.6-Complement-Rule-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>6.6 Complement Rule</a></div><div class=\"lev2 toc-item\"><a href=\"#6.7-Multiplication-Rule\" data-toc-modified-id=\"6.7-Multiplication-Rule-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>6.7 Multiplication Rule</a></div><div class=\"lev2 toc-item\"><a href=\"#6.8-Addition-Rule\" data-toc-modified-id=\"6.8-Addition-Rule-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>6.8 Addition Rule</a></div><div class=\"lev2 toc-item\"><a href=\"#6.9-Bayes’s-Law-Formula\" data-toc-modified-id=\"6.9-Bayes’s-Law-Formula-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>6.9 Bayes’s Law Formula</a></div><div class=\"lev1 toc-item\"><a href=\"#7.-Random-Variables-and-Discrete-Probability-Distributions\" data-toc-modified-id=\"7.-Random-Variables-and-Discrete-Probability-Distributions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>7. Random Variables and Discrete Probability Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#7.1-Describing-the-Population-Probability-Distribution\" data-toc-modified-id=\"7.1-Describing-the-Population-Probability-Distribution-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>7.1 Describing the Population Probability Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#7.2-Laws-of-Expected-Value-and-Variance\" data-toc-modified-id=\"7.2-Laws-of-Expected-Value-and-Variance-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>7.2 Laws of Expected Value and Variance</a></div><div class=\"lev2 toc-item\"><a href=\"#7.3-Bivariate-Distributions\" data-toc-modified-id=\"7.3-Bivariate-Distributions-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>7.3 Bivariate Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#7.4-Laws-of-Expected-Value-and-Variance-of-the-Sum-of-Two-Variables\" data-toc-modified-id=\"7.4-Laws-of-Expected-Value-and-Variance-of-the-Sum-of-Two-Variables-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>7.4 Laws of Expected Value and Variance of the Sum of Two Variables</a></div><div class=\"lev2 toc-item\"><a href=\"#7.5-Mean-and-Variance-of-a-Portfolio-of-Two-Stocks\" data-toc-modified-id=\"7.5-Mean-and-Variance-of-a-Portfolio-of-Two-Stocks-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>7.5 Mean and Variance of a Portfolio of Two Stocks</a></div><div class=\"lev2 toc-item\"><a href=\"#7.6-Portfolios-with-More-Than-Two-Stocks\" data-toc-modified-id=\"7.6-Portfolios-with-More-Than-Two-Stocks-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>7.6 Portfolios with More Than Two Stocks</a></div><div class=\"lev2 toc-item\"><a href=\"#7.7-Binormial-Distribution\" data-toc-modified-id=\"7.7-Binormial-Distribution-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>7.7 Binormial Distribution</a></div><div class=\"lev3 toc-item\"><a href=\"#7.7.1-Cumulative-Probability\" data-toc-modified-id=\"7.7.1-Cumulative-Probability-7.7.1\"><span class=\"toc-item-num\">7.7.1&nbsp;&nbsp;</span>7.7.1 Cumulative Probability</a></div><div class=\"lev3 toc-item\"><a href=\"#7.7.2-Binomial-Probability-p(X-≥-x)\" data-toc-modified-id=\"7.7.2-Binomial-Probability-p(X-≥-x)-7.7.2\"><span class=\"toc-item-num\">7.7.2&nbsp;&nbsp;</span>7.7.2 Binomial Probability p(X ≥ x)</a></div><div class=\"lev3 toc-item\"><a href=\"#7.7.3-Binomial-Probability-P(X-=-x)\" data-toc-modified-id=\"7.7.3-Binomial-Probability-P(X-=-x)-7.7.3\"><span class=\"toc-item-num\">7.7.3&nbsp;&nbsp;</span>7.7.3 Binomial Probability P(X = x)</a></div><div class=\"lev3 toc-item\"><a href=\"#7.7.4-Mean-and-Variance-of-a-Binomial-Distribution\" data-toc-modified-id=\"7.7.4-Mean-and-Variance-of-a-Binomial-Distribution-7.7.4\"><span class=\"toc-item-num\">7.7.4&nbsp;&nbsp;</span>7.7.4 Mean and Variance of a Binomial Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#7.8-Poisson-Distribution\" data-toc-modified-id=\"7.8-Poisson-Distribution-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>7.8 Poisson Distribution</a></div><div class=\"lev1 toc-item\"><a href=\"#8.-Continuous-Probability-Distributions\" data-toc-modified-id=\"8.-Continuous-Probability-Distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>8. Continuous Probability Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#8.1-Uniform-Distribution\" data-toc-modified-id=\"8.1-Uniform-Distribution-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>8.1 Uniform Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#8.2-Normal-Distribution\" data-toc-modified-id=\"8.2-Normal-Distribution-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>8.2 Normal Distribution</a></div><div class=\"lev3 toc-item\"><a href=\"#8.2.1-Calculating-Normal-Probabilities\" data-toc-modified-id=\"8.2.1-Calculating-Normal-Probabilities-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>8.2.1 Calculating Normal Probabilities</a></div><div class=\"lev2 toc-item\"><a href=\"#8.3-Exponential-Distribution\" data-toc-modified-id=\"8.3-Exponential-Distribution-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>8.3 Exponential Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#8.4-Student-t-Distribution\" data-toc-modified-id=\"8.4-Student-t-Distribution-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>8.4 Student t Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#8.5-Chi-Squared-Distribution\" data-toc-modified-id=\"8.5-Chi-Squared-Distribution-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>8.5 Chi-Squared Distribution</a></div><div class=\"lev2 toc-item\"><a href=\"#8.6-F-Distribution\" data-toc-modified-id=\"8.6-F-Distribution-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>8.6 F Distribution</a></div><div class=\"lev1 toc-item\"><a href=\"#9.-Sampling-Distributions\" data-toc-modified-id=\"9.-Sampling-Distributions-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>9. Sampling Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#9.1-Sampling-Distribution-of-the-Mean\" data-toc-modified-id=\"9.1-Sampling-Distribution-of-the-Mean-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>9.1 Sampling Distribution of the Mean</a></div><div class=\"lev2 toc-item\"><a href=\"#9.2-Sampling-Distribution-of-a-Sample-Proportion\" data-toc-modified-id=\"9.2-Sampling-Distribution-of-a-Sample-Proportion-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>9.2 Sampling Distribution of a Sample Proportion</a></div><div class=\"lev2 toc-item\"><a href=\"#9.3-Sampling-Distribution-of-the-Difference-between-Two-Means\" data-toc-modified-id=\"9.3-Sampling-Distribution-of-the-Difference-between-Two-Means-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>9.3 Sampling Distribution of the Difference between Two Means</a></div><div class=\"lev1 toc-item\"><a href=\"#10.-Introduction-to-Estimation\" data-toc-modified-id=\"10.-Introduction-to-Estimation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>10. Introduction to Estimation</a></div><div class=\"lev2 toc-item\"><a href=\"#10.1-Estimating-the-Population-Mean-When-the-Population-Standard-Deviation-is-Known\" data-toc-modified-id=\"10.1-Estimating-the-Population-Mean-When-the-Population-Standard-Deviation-is-Known-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>10.1 Estimating the Population Mean When the Population Standard Deviation is Known</a></div><div class=\"lev2 toc-item\"><a href=\"#10.2-Determining-the-Sample-Size-to-Estimate-$\\mu$\" data-toc-modified-id=\"10.2-Determining-the-Sample-Size-to-Estimate-$\\mu$-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>10.2 Determining the Sample Size to Estimate <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BC;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-2462\" role=\"math\" style=\"width: 0.646em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.532em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.782em 1000.49em 2.616em -999.998em); top: -2.309em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-2463\"><span class=\"mi\" id=\"MathJax-Span-2464\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span></span><span style=\"display: inline-block; width: 0px; height: 2.313em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.27em; border-left: 0px solid; width: 0px; height: 0.866em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>μ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">\\mu</script></a></div><div class=\"lev1 toc-item\"><a href=\"#11.-Introduction-to-Hypothesis-Testing\" data-toc-modified-id=\"11.-Introduction-to-Hypothesis-Testing-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>11. Introduction to Hypothesis Testing</a></div><div class=\"lev2 toc-item\"><a href=\"#11.1-Concepts-of-Hypothesis-Testing\" data-toc-modified-id=\"11.1-Concepts-of-Hypothesis-Testing-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>11.1 Concepts of Hypothesis Testing</a></div><div class=\"lev2 toc-item\"><a href=\"#11.2-Testing-the-Population-Mean-When-the-Population-Standard-Deviation-is-Known\" data-toc-modified-id=\"11.2-Testing-the-Population-Mean-When-the-Population-Standard-Deviation-is-Known-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>11.2 Testing the Population Mean When the Population Standard Deviation is Known</a></div><div class=\"lev3 toc-item\"><a href=\"#11.2.1-Standardized-Test-Statistic\" data-toc-modified-id=\"11.2.1-Standardized-Test-Statistic-11.2.1\"><span class=\"toc-item-num\">11.2.1&nbsp;&nbsp;</span>11.2.1 Standardized Test Statistic</a></div><div class=\"lev3 toc-item\"><a href=\"#11.2.2-Testing-Hypotheses-and-Confidence-Interval-Estimators\" data-toc-modified-id=\"11.2.2-Testing-Hypotheses-and-Confidence-Interval-Estimators-11.2.2\"><span class=\"toc-item-num\">11.2.2&nbsp;&nbsp;</span>11.2.2 Testing Hypotheses and Confidence Interval Estimators</a></div><div class=\"lev2 toc-item\"><a href=\"#11.3-Calculating-the-Probability-of-a-Type-II-Error\" data-toc-modified-id=\"11.3-Calculating-the-Probability-of-a-Type-II-Error-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>11.3 Calculating the Probability of a Type II Error</a></div><div class=\"lev2 toc-item\"><a href=\"#11.4-Larger-Sample-Size-Equals-More-Information-Equals-Better-Decisions\" data-toc-modified-id=\"11.4-Larger-Sample-Size-Equals-More-Information-Equals-Better-Decisions-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>11.4 Larger Sample Size Equals More Information Equals Better Decisions</a></div><div class=\"lev2 toc-item\"><a href=\"#11.5-Power-of-a-Test\" data-toc-modified-id=\"11.5-Power-of-a-Test-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>11.5 Power of a Test</a></div><div class=\"lev1 toc-item\"><a href=\"#12.-Inference-About-a-Population\" data-toc-modified-id=\"12.-Inference-About-a-Population-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>12. Inference About a Population</a></div><div class=\"lev2 toc-item\"><a href=\"#12.1-Inference-about-a-Population-Mean-When-the-Population-Standard-Deviation-is-Unknown\" data-toc-modified-id=\"12.1-Inference-about-a-Population-Mean-When-the-Population-Standard-Deviation-is-Unknown-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>12.1 Inference about a Population Mean When the Population Standard Deviation is Unknown</a></div><div class=\"lev2 toc-item\"><a href=\"#12.2-Inference-about-a-Population-Variance\" data-toc-modified-id=\"12.2-Inference-about-a-Population-Variance-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>12.2 Inference about a Population Variance</a></div><div class=\"lev2 toc-item\"><a href=\"#12.3-Inference-about-a-Population-Proportion\" data-toc-modified-id=\"12.3-Inference-about-a-Population-Proportion-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>12.3 Inference about a Population Proportion</a></div><div class=\"lev1 toc-item\"><a href=\"#13.-Inference-about-Comparing-Two-Populations\" data-toc-modified-id=\"13.-Inference-about-Comparing-Two-Populations-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>13. Inference about Comparing Two Populations</a></div><div class=\"lev2 toc-item\"><a href=\"#13.1-Inference-about-the-Difference-between-two-Means:-Independent-Samples\" data-toc-modified-id=\"13.1-Inference-about-the-Difference-between-two-Means:-Independent-Samples-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>13.1 Inference about the Difference between two Means: Independent Samples</a></div><div class=\"lev3 toc-item\"><a href=\"#13.1.1-Test-Statistic-for-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-=-\\sigma_{2}^2$\" data-toc-modified-id=\"13.1.1-Test-Statistic-for-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-=-\\sigma_{2}^2$-13.1.1\"><span class=\"toc-item-num\">13.1.1&nbsp;&nbsp;</span>13.1.1 Test Statistic for <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3208\" role=\"math\" style=\"width: 3.752em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.104em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.947em 1003.1em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3209\"><span class=\"msubsup\" id=\"MathJax-Span-3210\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3211\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3212\"><span class=\"mrow\" id=\"MathJax-Span-3213\"><span class=\"mn\" id=\"MathJax-Span-3214\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3215\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-3216\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3217\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3218\"><span class=\"mrow\" id=\"MathJax-Span-3219\"><span class=\"mn\" id=\"MathJax-Span-3220\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-142\">\\mu_{1} - \\mu_{2}</script> when <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3221\" role=\"math\" style=\"width: 4.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.382em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.53em 1003.38em 2.965em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3222\"><span class=\"msubsup\" id=\"MathJax-Span-3223\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3224\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3225\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3226\"><span class=\"mrow\" id=\"MathJax-Span-3227\"><span class=\"mn\" id=\"MathJax-Span-3228\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3229\" style=\"font-family: STIXMathJax_Main; padding-left: 0.326em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-3230\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3231\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3232\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3233\"><span class=\"mrow\" id=\"MathJax-Span-3234\"><span class=\"mn\" id=\"MathJax-Span-3235\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.442em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow><mn>2</mn></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">\\sigma_{1}^2 = \\sigma_{2}^2</script></a></div><div class=\"lev3 toc-item\"><a href=\"#13.1.2-Confidence-Interval-Estimator-of-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-=-\\sigma_{2}^2$\" data-toc-modified-id=\"13.1.2-Confidence-Interval-Estimator-of-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-=-\\sigma_{2}^2$-13.1.2\"><span class=\"toc-item-num\">13.1.2&nbsp;&nbsp;</span>13.1.2 Confidence Interval Estimator of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3366\" role=\"math\" style=\"width: 3.752em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.104em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.947em 1003.1em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3367\"><span class=\"msubsup\" id=\"MathJax-Span-3368\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3369\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3370\"><span class=\"mrow\" id=\"MathJax-Span-3371\"><span class=\"mn\" id=\"MathJax-Span-3372\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3373\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-3374\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3375\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3376\"><span class=\"mrow\" id=\"MathJax-Span-3377\"><span class=\"mn\" id=\"MathJax-Span-3378\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">\\mu_{1} - \\mu_{2}</script> when <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-148-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3379\" role=\"math\" style=\"width: 4.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.382em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.53em 1003.38em 2.965em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3380\"><span class=\"msubsup\" id=\"MathJax-Span-3381\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3382\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3383\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3384\"><span class=\"mrow\" id=\"MathJax-Span-3385\"><span class=\"mn\" id=\"MathJax-Span-3386\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3387\" style=\"font-family: STIXMathJax_Main; padding-left: 0.326em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-3388\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3389\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3390\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3391\"><span class=\"mrow\" id=\"MathJax-Span-3392\"><span class=\"mn\" id=\"MathJax-Span-3393\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.442em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow><mn>2</mn></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-148\">\\sigma_{1}^2 = \\sigma_{2}^2</script></a></div><div class=\"lev3 toc-item\"><a href=\"#13.1.3-Test-Statistic-for-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-\\ne-\\sigma_{2}^2$\" data-toc-modified-id=\"13.1.3-Test-Statistic-for-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-\\ne-\\sigma_{2}^2$-13.1.3\"><span class=\"toc-item-num\">13.1.3&nbsp;&nbsp;</span>13.1.3 Test Statistic for <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3452\" role=\"math\" style=\"width: 3.752em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.104em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.947em 1003.1em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3453\"><span class=\"msubsup\" id=\"MathJax-Span-3454\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3455\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3456\"><span class=\"mrow\" id=\"MathJax-Span-3457\"><span class=\"mn\" id=\"MathJax-Span-3458\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3459\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-3460\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3461\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3462\"><span class=\"mrow\" id=\"MathJax-Span-3463\"><span class=\"mn\" id=\"MathJax-Span-3464\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">\\mu_{1} - \\mu_{2}</script> when <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mn>2</mn></msubsup><mo>&amp;#x2260;</mo><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3465\" role=\"math\" style=\"width: 4.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.382em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.53em 1003.38em 2.965em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3466\"><span class=\"msubsup\" id=\"MathJax-Span-3467\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3468\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3469\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3470\"><span class=\"mrow\" id=\"MathJax-Span-3471\"><span class=\"mn\" id=\"MathJax-Span-3472\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3473\" style=\"font-family: STIXMathJax_Main; padding-left: 0.326em;\">≠</span><span class=\"msubsup\" id=\"MathJax-Span-3474\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3475\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3476\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3477\"><span class=\"mrow\" id=\"MathJax-Span-3478\"><span class=\"mn\" id=\"MathJax-Span-3479\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.442em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow><mn>2</mn></msubsup><mo>≠</mo><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow><mn>2</mn></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">\\sigma_{1}^2 \\ne \\sigma_{2}^2</script></a></div><div class=\"lev3 toc-item\"><a href=\"#13.1.4-Confidence-Interval-Estimator-of-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-\\ne-\\sigma_{2}^2$\" data-toc-modified-id=\"13.1.4-Confidence-Interval-Estimator-of-$\\mu_{1}---\\mu_{2}$-when-$\\sigma_{1}^2-\\ne-\\sigma_{2}^2$-13.1.4\"><span class=\"toc-item-num\">13.1.4&nbsp;&nbsp;</span>13.1.4 Confidence Interval Estimator of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3645\" role=\"math\" style=\"width: 3.752em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.104em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.947em 1003.1em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3646\"><span class=\"msubsup\" id=\"MathJax-Span-3647\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3648\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3649\"><span class=\"mrow\" id=\"MathJax-Span-3650\"><span class=\"mn\" id=\"MathJax-Span-3651\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3652\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-3653\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3654\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3655\"><span class=\"mrow\" id=\"MathJax-Span-3656\"><span class=\"mn\" id=\"MathJax-Span-3657\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">\\mu_{1} - \\mu_{2}</script> when <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mn>2</mn></msubsup><mo>&amp;#x2260;</mo><msubsup><mi>&amp;#x03C3;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3658\" role=\"math\" style=\"width: 4.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.382em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.53em 1003.38em 2.965em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-3659\"><span class=\"msubsup\" id=\"MathJax-Span-3660\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3661\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3662\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3663\"><span class=\"mrow\" id=\"MathJax-Span-3664\"><span class=\"mn\" id=\"MathJax-Span-3665\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-3666\" style=\"font-family: STIXMathJax_Main; padding-left: 0.326em;\">≠</span><span class=\"msubsup\" id=\"MathJax-Span-3667\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 1.021em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.51em 4.123em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-3668\" style=\"font-family: STIXMathJax_Main-italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.049em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -4.303em; left: 0.604em;\"><span class=\"mn\" id=\"MathJax-Span-3669\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; clip: rect(3.382em 1000.42em 4.123em -999.998em); top: -3.655em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-3670\"><span class=\"mrow\" id=\"MathJax-Span-3671\"><span class=\"mn\" id=\"MathJax-Span-3672\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.442em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow><mn>2</mn></msubsup><mo>≠</mo><msubsup><mi>σ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow><mn>2</mn></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">\\sigma_{1}^2 \\ne \\sigma_{2}^2</script></a></div><div class=\"lev3 toc-item\"><a href=\"#13.1.5-Testing-the-Population-Variances\" data-toc-modified-id=\"13.1.5-Testing-the-Population-Variances-13.1.5\"><span class=\"toc-item-num\">13.1.5&nbsp;&nbsp;</span>13.1.5 Testing the Population Variances</a></div><div class=\"lev2 toc-item\"><a href=\"#13.2-Inference-about-the-Difference-between-two-Means:-Matched-Pairs-Experiment\" data-toc-modified-id=\"13.2-Inference-about-the-Difference-between-two-Means:-Matched-Pairs-Experiment-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>13.2 Inference about the Difference between two Means: Matched Pairs Experiment</a></div><div class=\"lev2 toc-item\"><a href=\"#13.3-Inference-about-the-Difference-between-two-Population-Proportions\" data-toc-modified-id=\"13.3-Inference-about-the-Difference-between-two-Population-Proportions-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>13.3 Inference about the Difference between two Population Proportions</a></div><div class=\"lev3 toc-item\"><a href=\"#13.3.1-Test-Statistic-for-$p_{1}-−-p_{2}$:-Case-1\" data-toc-modified-id=\"13.3.1-Test-Statistic-for-$p_{1}-−-p_{2}$:-Case-1-13.3.1\"><span class=\"toc-item-num\">13.3.1&nbsp;&nbsp;</span>13.3.1 Test Statistic for <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-185-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4395\" role=\"math\" style=\"width: 3.613em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.012em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.9em 1003.01em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-4396\"><span class=\"msubsup\" id=\"MathJax-Span-4397\"><span style=\"display: inline-block; position: relative; width: 0.928em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-4398\" style=\"font-family: STIXMathJax_Main-italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-4399\"><span class=\"mrow\" id=\"MathJax-Span-4400\"><span class=\"mn\" id=\"MathJax-Span-4401\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-4402\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-4403\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.928em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-4404\" style=\"font-family: STIXMathJax_Main-italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-4405\"><span class=\"mrow\" id=\"MathJax-Span-4406\"><span class=\"mn\" id=\"MathJax-Span-4407\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-185\">p_{1} − p_{2}</script>: Case 1</a></div><div class=\"lev3 toc-item\"><a href=\"#13.3.2-Test-Statistic-for-$p_{1}-−-p_{2}$:-Case-2\" data-toc-modified-id=\"13.3.2-Test-Statistic-for-$p_{1}-−-p_{2}$:-Case-2-13.3.2\"><span class=\"toc-item-num\">13.3.2&nbsp;&nbsp;</span>13.3.2 Test Statistic for <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-190-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4521\" role=\"math\" style=\"width: 3.613em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.012em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.9em 1003.01em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-4522\"><span class=\"msubsup\" id=\"MathJax-Span-4523\"><span style=\"display: inline-block; position: relative; width: 0.928em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-4524\" style=\"font-family: STIXMathJax_Main-italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-4525\"><span class=\"mrow\" id=\"MathJax-Span-4526\"><span class=\"mn\" id=\"MathJax-Span-4527\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-4528\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-4529\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.928em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-4530\" style=\"font-family: STIXMathJax_Main-italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-4531\"><span class=\"mrow\" id=\"MathJax-Span-4532\"><span class=\"mn\" id=\"MathJax-Span-4533\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-190\">p_{1} − p_{2}</script>: Case 2</a></div><div class=\"lev1 toc-item\"><a href=\"#14.-Analysis-of-Variance\" data-toc-modified-id=\"14.-Analysis-of-Variance-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>14. Analysis of Variance</a></div><div class=\"lev2 toc-item\"><a href=\"#14.1-One-way-Analysis-of-Variance\" data-toc-modified-id=\"14.1-One-way-Analysis-of-Variance-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>14.1 One-way Analysis of Variance</a></div><div class=\"lev3 toc-item\"><a href=\"#14.1.1-Can-We-Use-the-t-Test-of-the-Difference-between-Two-Means-Instead-of-the-Analysis-of-Variance?\" data-toc-modified-id=\"14.1.1-Can-We-Use-the-t-Test-of-the-Difference-between-Two-Means-Instead-of-the-Analysis-of-Variance?-14.1.1\"><span class=\"toc-item-num\">14.1.1&nbsp;&nbsp;</span>14.1.1 Can We Use the t-Test of the Difference between Two Means Instead of the Analysis of Variance?</a></div><div class=\"lev3 toc-item\"><a href=\"#14.1.2-Can-We-Use-the-Analysis-of-Variance-Instead-of-the-t-Test-of-$\\mu_{1}-−-\\mu_{2}$?\" data-toc-modified-id=\"14.1.2-Can-We-Use-the-Analysis-of-Variance-Instead-of-the-t-Test-of-$\\mu_{1}-−-\\mu_{2}$?-14.1.2\"><span class=\"toc-item-num\">14.1.2&nbsp;&nbsp;</span>14.1.2 Can We Use the Analysis of Variance Instead of the t-Test of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-208-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>&amp;#x03BC;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-5057\" role=\"math\" style=\"width: 3.752em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.104em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.947em 1003.1em 2.826em -999.998em); top: -2.498em; left: 0.002em;\"><span class=\"mrow\" id=\"MathJax-Span-5058\"><span class=\"msubsup\" id=\"MathJax-Span-5059\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-5060\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-5061\"><span class=\"mrow\" id=\"MathJax-Span-5062\"><span class=\"mn\" id=\"MathJax-Span-5063\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-5064\" style=\"font-family: STIXMathJax_Main; padding-left: 0.234em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-5065\" style=\"padding-left: 0.234em;\"><span style=\"display: inline-block; position: relative; width: 0.975em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.428em 1000.47em 4.308em -999.998em); top: -3.979em; left: 0.002em;\"><span class=\"mi\" id=\"MathJax-Span-5066\" style=\"font-family: STIXMathJax_Main-italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.512em;\"><span class=\"texatom\" id=\"MathJax-Span-5067\"><span class=\"mrow\" id=\"MathJax-Span-5068\"><span class=\"mn\" id=\"MathJax-Span-5069\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.502em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>μ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-208\">\\mu_{1} − \\mu_{2}</script>?</a></div><div class=\"lev2 toc-item\"><a href=\"#14.2-Multiple-Comparisions\" data-toc-modified-id=\"14.2-Multiple-Comparisions-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>14.2 Multiple Comparisions</a></div><div class=\"lev2 toc-item\"><a href=\"#14.3-Analysis-of-Variance-Experimental-Designs\" data-toc-modified-id=\"14.3-Analysis-of-Variance-Experimental-Designs-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>14.3 Analysis of Variance Experimental Designs</a></div><div class=\"lev3 toc-item\"><a href=\"#14.3.1-Single-Factor-and-Multifactor-Experimental-Designs\" data-toc-modified-id=\"14.3.1-Single-Factor-and-Multifactor-Experimental-Designs-14.3.1\"><span class=\"toc-item-num\">14.3.1&nbsp;&nbsp;</span>14.3.1 Single-Factor and Multifactor Experimental Designs</a></div><div class=\"lev3 toc-item\"><a href=\"#14.3.2-Independent-Samples-and-Blocks\" data-toc-modified-id=\"14.3.2-Independent-Samples-and-Blocks-14.3.2\"><span class=\"toc-item-num\">14.3.2&nbsp;&nbsp;</span>14.3.2 Independent Samples and Blocks</a></div><div class=\"lev3 toc-item\"><a href=\"#14.3.3-Fixed-and-Random-Effects\" data-toc-modified-id=\"14.3.3-Fixed-and-Random-Effects-14.3.3\"><span class=\"toc-item-num\">14.3.3&nbsp;&nbsp;</span>14.3.3 Fixed and Random Effects</a></div><div class=\"lev2 toc-item\"><a href=\"#14.4-Randomized-Block-(Two-Way)-Analysis-of-Variance\" data-toc-modified-id=\"14.4-Randomized-Block-(Two-Way)-Analysis-of-Variance-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>14.4 Randomized Block (Two-Way) Analysis of Variance</a></div><div class=\"lev2 toc-item\"><a href=\"#14.5-Two-Factor-Analysis-of-Variance\" data-toc-modified-id=\"14.5-Two-Factor-Analysis-of-Variance-14.5\"><span class=\"toc-item-num\">14.5&nbsp;&nbsp;</span>14.5 Two-Factor Analysis of Variance</a></div><div class=\"lev1 toc-item\"><a href=\"#15.-Chi-Squared-Tests\" data-toc-modified-id=\"15.-Chi-Squared-Tests-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>15. Chi-Squared Tests</a></div><div class=\"lev2 toc-item\"><a href=\"#15.1-Chi-Squared-Goodness-Of-Fit-Test\" data-toc-modified-id=\"15.1-Chi-Squared-Goodness-Of-Fit-Test-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>15.1 Chi-Squared Goodness-Of-Fit Test</a></div><div class=\"lev2 toc-item\"><a href=\"#15.2-Chi-Squared-Test-of-a-Contingency-Table\" data-toc-modified-id=\"15.2-Chi-Squared-Test-of-a-Contingency-Table-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>15.2 Chi-Squared Test of a Contingency Table</a></div><div class=\"lev1 toc-item\"><a href=\"#16.-Simple-linear-regression-and-correlation\" data-toc-modified-id=\"16.-Simple-linear-regression-and-correlation-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>16. Simple linear regression and correlation</a></div><div class=\"lev2 toc-item\"><a href=\"#16.1-Testing-the-Slope\" data-toc-modified-id=\"16.1-Testing-the-Slope-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;</span>16.1 Testing the Slope</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Statistics?\n",
    "\n",
    "- **population** is the group of all items of interest to a statistics practitioner. A descriptive measure of a population is called a **parameter**.\n",
    "- **sample** is a set of data drawn from the studied population. A descriptive measure of a sample is called a **statistic**.   \n",
    "\n",
    "# 2. Graphical Descriptive Techniques I\n",
    "\n",
    "## 2.1 Types of Data and Information\n",
    "\n",
    "- **Interval data** are real numbers, such as heights, weights, incomes, and distances. We also refer to this type of data as **quantitative** or **numerical**.\n",
    "- The values of **nominal data** are categories. the values are not numbers but instead are words that describe the categories. Nominal data are also called **qualitative** or **categorical**.\n",
    "- **Ordinal data** appear to be nominal, but the difference is that the order of their values has meaning.\n",
    "\n",
    "## 2.2 Describing a Set of Nominal Data\n",
    "\n",
    "- A **bar chart** is often used to display frequencies; \n",
    "- A **pie chart** graphically shows relative frequencies.\n",
    "- The bar chart focuses on the frequencies and the pie chart focuses on the proportions\n",
    "\n",
    "# 3. Graphical Descriptive Techniques II\n",
    "\n",
    "## 3.1 Describing a Set of Interval Data\n",
    "\n",
    "## 3.1.1 Histogram\n",
    "\n",
    "A **histogram** is created by drawing rectangles whose bases are the intervals and whose heights are the frequencies.\n",
    "\n",
    "## 3.1.1.1 Determining the Number of Class Intervals\n",
    "\n",
    "$$Number\\ Of\\ Class\\ Intervals = 1 + 3.3 log(n)$$ \n",
    "\n",
    "\n",
    "$$Class\\ Width = \\frac{Largest\\ Observation - Smallest\\ Observation}{Number\\ Of\\ Classes}$$\n",
    "\n",
    "## 3.1.2 Stem-and-Leaf Display\n",
    "\n",
    "The first step in developing a stem-and-leaf display is to split each observation into two parts, a stem and a leaf. There are several different ways of doing this. For example, the number 12.3 can be split so that the stem is 12 and the leaf is 3. Another method can define the stem as 1 and the leaf as 2 (ignoring the 3). After each stem, we list that stem’s leaves, usually in ascending order. The advantage of the stem-and-leaf display over the histogram is that we can see the\n",
    "actual observations.\n",
    "\n",
    "```\n",
    "Stem  Leaf\n",
    "0     000000000111112222223333345555556666666778888999999\n",
    "1     000001111233333334455555667889999\n",
    "2     0000111112344666778999\n",
    "3     001335589\n",
    "4     124445589\n",
    "5     022224556789\n",
    "```\n",
    "\n",
    "## 3.2 Describing time-series Data\n",
    "\n",
    "**cross-sectional data** classifies data by type, **time-series data** classifies them according to whether the observations are measured at the same time or whether they represent measurements at successive points in time. Time-series data are often graphically depicted on a **line chart**, which plots the value of the variable on the vertical axis and the time periods on the horizontal axis.    \n",
    "\n",
    "## 3.3 Describing the Relationship between Two Interval Data\n",
    "\n",
    "In applications where one variable depends to some degree on the other variable, we label the dependent\n",
    "variable Y and the other, called the independent variable, X. In interpreting the results of a scatter diagram it is important to understand that if two variables are linearly related it does not mean that one is causing the other. **Correlation is not causation**.\n",
    "\n",
    "# 4. Numerical Descriptive Techniques\n",
    "\n",
    "## 4.1 Measure of Central Location\n",
    "\n",
    "### 4.1.1 Arithmetic Mean\n",
    "\n",
    "$$\\mu=\\frac{\\sum_{i=1}^Nx_{i}}{N}$$\n",
    "\n",
    "$$\\bar x=\\frac{\\sum_{i=1}^nx_{i}}{n}$$\n",
    "\n",
    "### 4.1.2 Median\n",
    "\n",
    "The median is calculated by placing all the observations in order (ascending or descending). The observation that falls in the middle is the median. When there is an even number of observations, the median is determined by averaging the two observations in the middle.\n",
    "\n",
    "### 4.1.3 Mode\n",
    "\n",
    "The mode is defined as the observation (or observations) that occurs with the greatest frequency.\n",
    "\n",
    "### 4.1.4 Mean, Median, Mode: Which Is Best?\n",
    "\n",
    "The mean is generally our first selection. One advantage the median holds is that it is not as sensitive to extreme values as is the mean. The mode is seldom the best measure of central location.\n",
    "\n",
    "### 4.1.5 Geometric mean\n",
    "\n",
    "$$(1+R_{g})^n=(1+R_{1})(1+R_{2})\\cdots(1+R{n})$$\n",
    "\n",
    "## 4.2 Measures of Variability\n",
    "\n",
    "### 4.2.1 Range\n",
    "\n",
    "```\n",
    "Range = Largest observation − Smallest observation\n",
    "```\n",
    "\n",
    "### 4.2.2 Variance\n",
    "\n",
    "$$\\sigma^2=\\frac{\\sum_{i=1}^N(x_{i}-\\mu)^2}{N}$$\n",
    "\n",
    "$$s^2=\\frac{\\sum_{i=1}^n(x_{i}-\\bar x)^2}{n-1}$$\n",
    "\n",
    "### 4.2.3 Standard Deviation\n",
    "\n",
    "$$\\sigma = \\sqrt{\\sigma ^2}$$\n",
    "$$s = \\sqrt{s ^2}$$\n",
    "\n",
    "### 4.2.4 Chebysheff’s Theorem\n",
    "\n",
    "The proportion of observations in any sample or population that lie within k standard deviations of the mean is at least\n",
    "\n",
    "$$1 - \\frac{1}{k^2}, k>1$$\n",
    "\n",
    "### 4.2.5 Coefficient of Variation\n",
    "\n",
    "$$CV = \\frac{\\sigma}{\\mu}$$\n",
    "\n",
    "$$cv = \\frac{s}{\\bar x}$$\n",
    "\n",
    "## 4.3 Measures of Relative Standing and Box Plots\n",
    "\n",
    "### 4.3.1 Percentile\n",
    "\n",
    "The $P_{th}$ percentile is the value for which P percent are less than that value and (100 – P)% are greater than that value.\n",
    "\n",
    "### 4.3.2 Locating Percentiles\n",
    "\n",
    "$$L_{p} = (n+1)\\frac{P}{100}$$\n",
    "\n",
    "where $L_{p}$ is the location of the $P_{th}$ percentile.        \n",
    "\n",
    "Placing the 10 observations in ascending order we get\n",
    "\n",
    "```\n",
    "0 0 5 7 8 9 12 14 22 33\n",
    "```\n",
    "\n",
    "The location of the 25th percentile is   \n",
    "\n",
    "$$L_{25} = (10+1)\\frac{25}{100} = 2.75$$\n",
    "\n",
    "The $25_{th}$ percentile is three-quarters of the distance between the second (which is 0) and the third (which is 5) observations. Three-quarters of the distance is\n",
    "\n",
    "```\n",
    "(.75)(5 − 0) = 3.75\n",
    "```\n",
    "\n",
    "Because the second observation is 0, the $25_{th}$ percentile is 0 + 3.75 = 3.75.\n",
    "\n",
    "### 4.3.3 Interquartile Range\n",
    "\n",
    "$$Interquartile\\ Range = Q_{3} − Q_{1}$$\n",
    "\n",
    "### 4.3.4 Box Plots\n",
    "\n",
    "This technique graphs five statistics: the minimum and maximum observations, and the first, second, and third quartiles. The three vertical lines of the box are the first, second, and third quartiles. The lines extending to the left and right are called **whiskers**. Any points that lie outside the whiskers are called **outliers**. The whiskers extend outward to the smaller of 1.5 times the interquartile range or to the most extreme point that is not an outlier.\n",
    "\n",
    "## 4.4 Measures of Linear Relationship\n",
    "\n",
    "### 4.4.1 Covariance\n",
    "\n",
    "$$\\sigma_{xy} = \\frac{\\sum_{i=1}^N(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{N}$$\n",
    "\n",
    "$$s_{xy} = \\frac{\\sum_{i=1}^n(x_{i}-\\bar x)(y_{i}-\\bar y)}{n-1}$$\n",
    "\n",
    "### 4.4.2 Coefficient of Correlation\n",
    "\n",
    "$$\\rho=\\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}$$\n",
    "\n",
    "$$r=\\frac{s_{xy}}{s_{x}s_{y}}$$\n",
    "\n",
    "### 4.4.3 Least Squares Method\n",
    "\n",
    "$$\\hat y = b_{0} + b_{1}x$$\n",
    "\n",
    "The coefficients $b_{0}$ and $b_{1}$ are derived using calculus so that we minimize the sum of squared deviations:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_{i}-\\hat{y_{i}})^2$$\n",
    "\n",
    "Least Squares Line Coefficients:   \n",
    "\n",
    "$$b_{1} = \\frac{s_{xy}}{s_{x}^2}$$\n",
    "$$b_{0} = \\bar y - b_{1}\\bar x$$\n",
    "\n",
    "### 4.4.4 Coefficient of Determination\n",
    "\n",
    "Coefficient of determination $R^2$ is calculated by squaring the coefficient of correlation. The coefficient of determination measures the amount of variation in the dependent variable that is explained by the variation in the independent variable.\n",
    "\n",
    "### 4.4.5 Interpreting Correlation\n",
    "\n",
    "**Correlation is not Causation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Data Collection And Sampling\n",
    "\n",
    "## 5.1 Simple Random Sample\n",
    "\n",
    "A simple random sample is a sample selected in such a way that every possible sample with the same number of observations is equally likely to be chosen.\n",
    "\n",
    "## 5.2 Stratified Random Sampling\n",
    "\n",
    "A stratified random sample is obtained by separating the population into mutually exclusive sets, or strata, and then drawing simple random samples from each stratum.\n",
    "\n",
    "## 5.3 Cluster Sampling\n",
    "\n",
    "A cluster sample is a simple random sample of groups or clusters of elements.\n",
    "\n",
    "## 5.4 Sampling Error\n",
    "\n",
    "Sampling error refers to differences between the sample and the population that exists only because of the observations that happened to be selected for the sample.\n",
    "\n",
    "## 5.5 Nonsampling Error\n",
    "\n",
    "Nonsampling errors result from mistakes made in the acquisition of data or from the sample observations being selected improperly.\n",
    "\n",
    "- Errors in data acquisition.\n",
    "- Nonresponse error refers to error (or bias) introduced when responses are not obtained from some members of the sample.\n",
    "- Selection bias occurs when the sampling plan is such that some members of the target population cannot possibly be selected for inclusion in the sample.\n",
    "\n",
    "# 6 Probability\n",
    "\n",
    "## 6.1 Intersection\n",
    "\n",
    "The intersection of events A and B is the event that occurs when both A and B occur. The probability of the intersection is called the **joint probability**.\n",
    "\n",
    "## 6.2 Marginal Probability\n",
    "\n",
    "Marginal probabilities, computed by adding across rows or down columns, are so named because they are calculated in the margins of the table.   \n",
    "\n",
    "## 6.3 Conditional Probability\n",
    "\n",
    "The probability of event A given event B is\n",
    "\n",
    "$$p(A|B) = \\frac{p(AB)}{p(B)}$$\n",
    "\n",
    "The probability of event B given event A is\n",
    "\n",
    "$$p(B|A) = \\frac{p(AB)}{p(A)}$$\n",
    "\n",
    "## 6.4 Independence\n",
    "\n",
    "Two events A and B are said to be independent if\n",
    "\n",
    "$$p(A|B) = p(A)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$p(B|A) = p(B)$$\n",
    "\n",
    "## 6.5 Union\n",
    "\n",
    "The union of events A and B is the event that occurs when either A or B or both occur. It is denoted as\n",
    "`A or B`.\n",
    "\n",
    "## 6.6 Complement Rule\n",
    "\n",
    "The complement of event A is the event that occurs when event A does not occur.\n",
    "\n",
    "$$p(A^c) = 1 - p(A)$$\n",
    "\n",
    "## 6.7 Multiplication Rule\n",
    "\n",
    "$$p(AB) = p(A)p(B|A) = p(B)p(A|B)$$\n",
    "\n",
    "## 6.8 Addition Rule\n",
    "\n",
    "The probability that event A, or event B, or both occur is\n",
    "\n",
    "$$p(A or B) = p(A) + p(B) - p(AB)$$\n",
    "\n",
    "## 6.9 Bayes’s Law Formula\n",
    "\n",
    "$$p(A_{i}|B) = \\frac{p(A_{i})p(B|A_{i})}{p(A_{1})p(B|A_{1}) + p(A_{2})p(B|A_{2}) + \\cdots + p(A_{k})p(B|A_{k})}$$\n",
    "\n",
    "# 7. Random Variables and Discrete Probability Distributions\n",
    "\n",
    "## 7.1 Describing the Population Probability Distribution\n",
    "\n",
    "$$E(x) = \\mu = \\sum xp(x)$$\n",
    "\n",
    "$$V(x) = \\sigma^2 = \\sum (x-\\mu)^2p(x)$$\n",
    "\n",
    "## 7.2 Laws of Expected Value and Variance\n",
    "\n",
    "$$E(c) = c$$\n",
    "$$E(x + c) = E(x) + c$$\n",
    "$$E(cx) = cE(x)$$\n",
    "$$V(c) = 0$$\n",
    "$$V(x + c) = V(x)$$\n",
    "$$V(cx) = c^2V(x)$$\n",
    "\n",
    "## 7.3 Bivariate Distributions\n",
    "\n",
    "The covariance of two discrete variables is defined as\n",
    "\n",
    "$$COV(x, y) = \\sigma_{xy} = \\sum \\sum (x - \\mu_{x})(y-\\mu_{y})p(x, y)$$\n",
    "\n",
    "Coefficient of Correlation:\n",
    "\n",
    "$$\\rho = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}$$\n",
    "\n",
    "## 7.4 Laws of Expected Value and Variance of the Sum of Two Variables\n",
    "\n",
    "$$E(x + y) = E(x) + E(y)$$\n",
    "\n",
    "$$V(x + y) = V(x) + V(y) + 2COV(x + y)$$\n",
    "\n",
    "## 7.5 Mean and Variance of a Portfolio of Two Stocks\n",
    "\n",
    "$$E(R_{p}) = w_{1}E(R_{1}) + w_{2}E(R_{2})$$\n",
    "\n",
    "$$V(R_{p}) = w_{1}^2V(R_{1}) + w_{2}^2V(R_{2}) + 2w_{1}w_{2}COV(R_{1}, R_{2}) = w_{1}^2V(R_{1}) + w_{2}^2V(R_{2}) + 2w_{1}w_{2}\\rho\\sigma_{1}\\sigma_{2}$$\n",
    "\n",
    "## 7.6 Portfolios with More Than Two Stocks\n",
    "\n",
    "$$E(R_{p}) = \\sum_{i=1}^k w_{i}E(R_{i})$$\n",
    "\n",
    "$$V(R_{p}) = \\sum_{i=1}^k w_{i}^2V(R_{i}) + 2\\sum_{i=1}^k \\sum_{j=i+1}^k w_{i}w_{j}COV(R_{i}, R_{j})$$\n",
    "\n",
    "## 7.7 Binormial Distribution\n",
    "\n",
    "- The binomial experiment consists of a fixed number of trials (n).\n",
    "- Each trial has two possible outcomes. success or failure.\n",
    "- The probability of success is p. The probability of failure is 1 − p.\n",
    "- The trials are independent\n",
    "\n",
    "The probability of x successes in a binomial experiment with n trials and probability of success = p is\n",
    "\n",
    "$$p(x) = \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$\n",
    "\n",
    "### 7.7.1 Cumulative Probability\n",
    "\n",
    "$$p(X \\le 4) = p(0) + p(1) + p(2) + p(3) + p(4)$$\n",
    "\n",
    "### 7.7.2 Binomial Probability p(X ≥ x)\n",
    "\n",
    "$$p(X \\ge x) = 1 - p(X \\le (x-1))$$\n",
    "\n",
    "### 7.7.3 Binomial Probability P(X = x)\n",
    "\n",
    "$$p(x) = p(X \\le x) - p(X \\le (x-1))$$\n",
    "\n",
    "### 7.7.4 Mean and Variance of a Binomial Distribution\n",
    "\n",
    "$$\\mu = np$$\n",
    "$$\\sigma^2 = np(1-p)$$\n",
    "$$\\sigma = \\sqrt{np(1-p)}$$\n",
    "\n",
    "## 7.8 Poisson Distribution\n",
    "\n",
    "Like the binomial random variable, the Poisson random variable is the number of occurrences of events, which we’ll continue to call successes. The difference between the two random variables is that a binomial random variable is the number of successes in a set number of trials, whereas a Poisson random variable is the number of successes in an interval of time or specific region of space.\n",
    "\n",
    "- The number of successes that occur in any interval is independent of the number of successes that occur in any other interval.\n",
    "- The probability of a success in an interval is the same for all equal-size intervals.\n",
    "- The probability of a success in an interval is proportional to the size of the interval.\n",
    "- The probability of more than one success in an interval approaches 0 as the interval becomes smaller.\n",
    "\n",
    "The probability that a Poisson random variable assumes a value of x in a specific interval is\n",
    "\n",
    "$$p(x) = \\frac{e^{-\\mu}\\mu^x}{x!}$$\n",
    "\n",
    "the variance of a Poisson random variable is equal to its mean; that is\n",
    "\n",
    "$$\\sigma^2 = \\mu$$\n",
    "\n",
    "$$p(X \\ge x) = 1 - p(X \\le (x-1))$$\n",
    "\n",
    "$$p(x) = p(X \\le x) - p(X \\le (x-1))$$\n",
    "\n",
    "# 8. Continuous Probability Distributions\n",
    "\n",
    "## 8.1 Uniform Distribution\n",
    "\n",
    "$$f(x) = \\frac{1}{b-a}, a \\le x \\le b$$\n",
    "\n",
    "## 8.2 Normal Distribution\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n",
    "\n",
    "### 8.2.1 Calculating Normal Probabilities\n",
    "\n",
    "We standardize a random variable by subtracting its mean and dividing by its standard deviation. When the variable\n",
    "is normal, the transformed variable is called a standard normal random variable and denoted by Z; that is,\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "## 8.3 Exponential Distribution\n",
    "\n",
    "$$f(x) = \\lambda e^{-\\lambda x}, x \\ge 0$$\n",
    "\n",
    "$$\\mu = \\sigma = \\frac{1}{\\lambda}$$\n",
    "\n",
    "$$p(X > x) = e^{-\\lambda x}$$\n",
    "\n",
    "$$p(X < x) = 1 - e^{-\\lambda x}$$\n",
    "\n",
    "$$p(x_{1} < X < x_{2}) = p(X < x_{2}) - p(X < x_{1}) = e^{-\\lambda x_{1}} - e^{-\\lambda x_{2}}$$\n",
    "\n",
    "\n",
    "## 8.4 Student t Distribution\n",
    "\n",
    "$$f(t)=\\frac{\\Gamma[(\\nu + 1)/2]}{\\sqrt{\\nu \\pi} \\Gamma (\\nu /2)}[1 + \\frac{t^2}{\\nu}]^{-(\\nu + 1)/2}$$\n",
    "\n",
    "where $\\nu$ (Greek letter nu) is the parameter of the Student t distribution called the **degrees of freedom**, and $\\Gamma$ is the gamma function.\n",
    "\n",
    "$$E(t) = 0$$\n",
    "\n",
    "$$V(t) = \\frac{\\nu}{\\nu - 2}, \\nu \\gt 2$$\n",
    "\n",
    "Student t distribution is similar to the standard normal distribution. Both are symmetrical about 0. We describe the Student t distribution as mound shaped, whereas the normal distribution is bell shaped. As $\\nu$ grows larger, the Student t distribution approaches the standard normal distribution.\n",
    "\n",
    "## 8.5 Chi-Squared Distribution\n",
    "\n",
    "$$f(\\chi^2) = \\frac{1}{\\Gamma(\\nu/2)} \\frac{1}{2^{\\nu/2}}(\\chi^2)^{(\\nu/2)-1}e^{-\\chi^2/2}$$\n",
    "\n",
    "$$E(\\chi^2) = \\nu$$\n",
    "\n",
    "$$V(\\chi^2) = 2\\nu$$\n",
    "\n",
    "## 8.6 F Distribution\n",
    "\n",
    "$$E(F) = \\frac{\\nu_{2}}{\\nu_{2} - 2}, \\nu_{2} \\gt 2$$\n",
    "\n",
    "$$V(F) = \\frac{2\\nu_{2}^2(\\nu_{1} + \\nu_{2} -2)}{\\nu_{1}(\\nu_{2}-1)^2(\\nu_{2} -4)}, \\nu_{2} \\gt 4$$\n",
    "\n",
    "$\\nu_{1}$ the numerator degrees of freedom and $\\nu_{2}$ the denominator degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sampling Distributions\n",
    "\n",
    "## 9.1 Sampling Distribution of the Mean\n",
    "\n",
    "**Central Limit Theorem**: The sampling distribution of the mean of a random sample drawn from any population is approximately normal for a sufficiently large sample size. The larger the sample size, the more closely the sampling distribution of X will resemble a normal distribution.\n",
    "\n",
    "$$\\mu_{\\bar x} = \\mu$$\n",
    "\n",
    "$$\\sigma_{\\bar x}^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "If X is normal, then $\\bar X$ is normal. If X is nonnormal, then $\\bar X$ is approximately normal for sufficiently large sample sizes. The definition of “sufficiently large” depends on the extent of nonnormality of X.\n",
    "\n",
    "Standardizing the sample mean:\n",
    "\n",
    "$$Z = \\frac{\\bar X - \\mu}{\\sigma / \\sqrt{n}}$$\n",
    "\n",
    "## 9.2 Sampling Distribution of a Sample Proportion\n",
    "\n",
    "$\\hat P$ is approximately normally distributed provided that np and n(1 − p) are greater than or equal to 5.\n",
    "\n",
    "$$E(\\hat P) = p$$\n",
    "\n",
    "$$V(\\hat P) = \\sigma_{\\hat p}^2 = \\frac{p(1-p)}{n}$$\n",
    "\n",
    "Standardizing the sample proportion:\n",
    "\n",
    "$$Z = \\frac{\\hat P - p}{\\sqrt{p(1-p)/n}}$$\n",
    "\n",
    "## 9.3 Sampling Distribution of the Difference between Two Means\n",
    "\n",
    "$$E(\\bar X_{1} - \\bar X_{2}) = \\mu_{\\bar x_{1} - \\bar x_{2}} = \\mu_{1} - \\mu_{2}$$\n",
    "\n",
    "$$V(\\bar X_{1} - \\bar X_{2}) = \\sigma_{\\bar x_{1} - \\bar x_{2}}^2 = \\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}$$\n",
    "\n",
    "Standardizing the difference between two sample means:\n",
    "\n",
    "$$Z = \\frac{(\\bar X_{1} - \\bar X_{2}) - (\\mu_{1} - \\mu_{2})}{\\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}}$$\n",
    "\n",
    "# 10. Introduction to Estimation\n",
    "\n",
    "- An **unbiased estimator** of a population parameter is an estimator whose expected value is equal to that parameter.\n",
    "- An unbiased estimator is said to be **consistent** if the difference between the estimator and the parameter grows smaller as the sample size grows larger.\n",
    "- If there are two unbiased estimators of a parameter, the one whose variance is smaller is said to have **relative efficiency**.\n",
    "\n",
    "## 10.1 Estimating the Population Mean When the Population Standard Deviation is Known\n",
    "\n",
    "$$\\bar x \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "## 10.2 Determining the Sample Size to Estimate $\\mu$\n",
    "\n",
    "$$n = (\\frac{z_{\\alpha/2}\\sigma}{B})^2$$\n",
    "\n",
    "$$B = Z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "B stands for the **bound on the error of estimation**.\n",
    "\n",
    "# 11. Introduction to Hypothesis Testing\n",
    "\n",
    "## 11.1 Concepts of Hypothesis Testing\n",
    "\n",
    "- **null hypothesis** usually refers to a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. $H_{0}$\n",
    "- **alternative hypothesis** (or **maintained hypothesis** or **research hypothesis**) refers the hypothesis to be accepted if the null hypothesis is rejected. $H_{1}$\n",
    "- A **Type I error** occurs when we reject a true null hypothesis. $\\alpha$\n",
    "- A **Type II error** is defined as not rejecting a false null hypothesis. $\\beta$\n",
    "- The **p-value** of a test is the probability of observing a test statistic at least as extreme as the one computed given that the null hypothesis is true.\n",
    "- If we reject the null hypothesis, we conclude that there is enough statistical evidence to infer that the alternative hypothesis is true. \n",
    "- If we do not reject the null hypothesis, we conclude that there is not enough statistical evidence to infer that the alternative hypothesis is true.\n",
    "\n",
    "## 11.2 Testing the Population Mean When the Population Standard Deviation is Known\n",
    "\n",
    "- A two-tail test is conducted whenever the alternative hypothesis specifies that the mean is not equal to the value stated in the null hypothesis.\n",
    "- a one-tail test that focuses on the right tail of the sampling distribution whenever we want to know whether there is enough evidence to infer that the mean is greater than the quantity specified by the null hypothesis.\n",
    "- a one-tail test that focuses on the left tail of the sampling distribution whenever we want to know whether there is enough evidence to infer that the mean is less than the quantity specified by the null hypothesis.\n",
    "\n",
    "### 11.2.1 Standardized Test Statistic\n",
    "\n",
    "$$z = \\frac{\\bar x - \\mu}{\\sigma / \\sqrt{n}}$$\n",
    "\n",
    "The rejection region:\n",
    "\n",
    "$$z > z_{\\alpha / 2}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$z < - z_{\\alpha / 2}$$\n",
    "\n",
    "### 11.2.2 Testing Hypotheses and Confidence Interval Estimators\n",
    "\n",
    "$$\\bar x \\pm z_{\\alpha / 2}\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "we compute the interval estimate and determine whether the hypothesized value of the mean falls into the interval.\n",
    "\n",
    "## 11.3 Calculating the Probability of a Type II Error\n",
    "\n",
    "Example: A random sample of 400 monthly accounts is drawn, for which the sample mean is \\$178. The accounts are approximately normally distributed with a standard deviation of \\$65. Whether the mean is greater than \\$170 with $\\alpha$ = 5%?\n",
    "\n",
    "$H_{0}$: $\\mu \\le 170$   \n",
    "\n",
    "$H_{1}$: $\\mu \\gt 170$    \n",
    "\n",
    "$\\frac{\\bar x_{L} - 170}{65/\\sqrt{400}} = 1.645$\n",
    "\n",
    "$\\bar x_{L} = 175.34$\n",
    "\n",
    "Therefore, the rejection region is:\n",
    "\n",
    "$\\bar x \\gt 175.34$\n",
    "\n",
    "The sample mean was computed to be 178. Because the test statistic (sample mean) is in the rejection region (it is greater than 175.34), we reject the null hypothesis. Thus, there is sufficient evidence to infer that the mean monthly account is greater than $170.\n",
    "\n",
    "$\\beta = P(\\bar X \\lt 175.34$, given that the null hypothesis is false )    \n",
    "\n",
    "Suppose that when the mean account is at least $180.    \n",
    "\n",
    "$\\beta = P(\\bar X \\lt 175.34$, given that $\\mu = 180)$    \n",
    "\n",
    "$\\beta = P(\\frac{\\bar X - \\mu}{\\sigma / \\sqrt{n}} < \\frac{175.34-180}{65/\\sqrt{400}}) = P(Z \\lt - 1.43) = 0.0764$    \n",
    "\n",
    "![](fig/alpha-beta.png)\n",
    "\n",
    "This plot illustrates the inverse relationship between the probabilities of Type I and Type II errors. Unfortunately, there is no simple formula to determine what the significance level should be.\n",
    "\n",
    "## 11.4 Larger Sample Size Equals More Information Equals Better Decisions\n",
    "\n",
    "## 11.5 Power of a Test\n",
    "\n",
    "**power**: the probability of its leading us to reject the null hypothesis when it is false. Thus, the power of a test\n",
    "is 1 − β.\n",
    "\n",
    "# 12. Inference About a Population\n",
    "\n",
    "## 12.1 Inference about a Population Mean When the Population Standard Deviation is Unknown\n",
    "\n",
    "When the population standard deviation is unknown and the population is normal, the test statistic for testing hypotheses about μ is\n",
    "\n",
    "$$t = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}$$\n",
    "\n",
    "which is Student t-distributed with ν = n − 1 degrees of freedom.    \n",
    "\n",
    "Confidence Interval Estimator of μ When σ Is Unknown     \n",
    "\n",
    "$$\\bar x \\pm t_{\\alpha/2}\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "## 12.2 Inference about a Population Variance\n",
    "\n",
    "The test statistic used to test hypotheses about $\\sigma^2$ is\n",
    "\n",
    "$$\\chi^2 = \\frac{(n-1)s^2}{\\sigma^2}$$\n",
    "\n",
    "which is chi-squared distributed with ν = n − 1 degrees of freedom when the population random variable is normally distributed with variance equal to $\\sigma^2$.\n",
    "\n",
    "Confidence Interval Estimator of $\\sigma^2$\n",
    "\n",
    "Lower confidence limit (LCL) = $\\frac{(n-1)s^2}{\\chi_{\\alpha /2}^2}$      \n",
    "\n",
    "Upper confidence limit (UCL) = $\\frac{(n-1)s^2}{\\chi_{1-\\alpha /2}^2}$     \n",
    "\n",
    "## 12.3 Inference about a Population Proportion\n",
    "\n",
    "$$\\hat p = \\frac{x}{n}$$\n",
    "\n",
    "Test Statistic for p\n",
    "\n",
    "$$z = \\frac{\\hat P - p}{\\sqrt{p(1-p)/n}}$$\n",
    "\n",
    "which is approximately normal when np and n(1 − p) are greater than 5.    \n",
    "\n",
    "Confidence Interval Estimator of p\n",
    "\n",
    "$$\\hat p \\pm z_{\\alpha /2} \\sqrt{\\hat p (1 - \\hat p)/n}$$\n",
    "\n",
    "Sample Size to Estimate a Proportion\n",
    "\n",
    "$$n = (\\frac{z_{\\alpha /2}\\sqrt{\\hat p (1-\\hat p)}}{B})^2$$\n",
    "\n",
    "$$B = z_{\\alpha /2} \\sqrt{\\frac{\\hat p (1-\\hat p)}{n}}$$\n",
    "\n",
    "# 13. Inference about Comparing Two Populations\n",
    "\n",
    "## 13.1 Inference about the Difference between two Means: Independent Samples\n",
    "\n",
    "Sampling Distribution of $\\bar x_{1} - \\bar x_{2}$:     \n",
    "\n",
    "$\\bar x_{1} - \\bar x_{2}$ is normally distributed if the populations are normal and approximately normal if the populations are nonnormal and the sample sizes are large.   \n",
    "$$E( \\bar x_{1} - \\bar x_{2} ) = \\mu_{1} - \\mu_{2}$$\n",
    "$$V( \\bar x_{1} - \\bar x_{2} ) = \\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}$$\n",
    "$$Z = \\frac{(\\bar x_{1} - \\bar x_{2}) -(\\mu_{1} - \\mu_{2})}{\\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}}$$\n",
    "\n",
    "### 13.1.1 Test Statistic for $\\mu_{1} - \\mu_{2}$ when $\\sigma_{1}^2 = \\sigma_{2}^2$\n",
    "\n",
    "$$t = \\frac{(\\bar x_{1} - \\bar x_{2}) -(\\mu_{1} - \\mu_{2})}{\\sqrt{s_{p}^2(\\frac{1}{n_{1}} + \\frac{1}{n_{2}})}}$$\n",
    "\n",
    "where $s_{p}^2$ is called the pooled variance estimator:\n",
    "\n",
    "$$s_{p}^2 = \\frac{(n_{1} -1)s_{1}^2 + (n_{2} -1)s_{2}^2}{n_{1} + n_{2} - 2}$$\n",
    "\n",
    "### 13.1.2 Confidence Interval Estimator of $\\mu_{1} - \\mu_{2}$ when $\\sigma_{1}^2 = \\sigma_{2}^2$\n",
    "\n",
    "$$(\\bar x_{1} - \\bar x_{2}) \\pm t_{\\alpha /2}\\sqrt{s_{p}^2(\\frac{1}{n_{1}} + \\frac{1}{n_{2}})}$$\n",
    "\n",
    "### 13.1.3 Test Statistic for $\\mu_{1} - \\mu_{2}$ when $\\sigma_{1}^2 \\ne \\sigma_{2}^2$\n",
    "\n",
    "$$t = \\frac{(\\bar x_{1} - \\bar x_{2}) -(\\mu_{1} - \\mu_{2})}{\\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}}$$\n",
    "\n",
    "$$\\nu = \\frac{(s_{1}^2/n_{1} + s_{2}^2/n_{2})^2}{\\frac{(s_{1}^2/n_{1})^2}{n_{1}-1} + \\frac{(s_{2}^2/n_{2})^2}{n_{2}-1}}$$\n",
    "\n",
    "### 13.1.4 Confidence Interval Estimator of $\\mu_{1} - \\mu_{2}$ when $\\sigma_{1}^2 \\ne \\sigma_{2}^2$\n",
    "\n",
    "$$(\\bar x_{1} - \\bar x_{2}) \\pm t_{\\alpha /2}\\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}$$\n",
    "\n",
    "### 13.1.5 Testing the Population Variances\n",
    "\n",
    "$H_{0}$: $\\frac{\\sigma_{1}^2}{\\sigma_{2}^2} = 1$        \n",
    "$H_{1}$: $\\frac{\\sigma_{1}^2}{\\sigma_{2}^2} \\ne 1$          \n",
    "\n",
    "$$F = \\frac{s_{1}^2}{s_{2}^2}$$\n",
    "\n",
    "$\\nu_{1} = n_{1} - 1$ and $\\nu_{2} = n_{2} - 1$. This is a two-tail test so that the rejection region is $F \\gt F_{\\alpha/2, \\nu_{1},\\nu_{2}}$ or $F \\lt F_{1-\\alpha/2, \\nu_{1},\\nu_{2}}$.\n",
    "\n",
    "Confidence Interval Estimator of $\\sigma_{1}^2/\\sigma_{2}^2$\n",
    "\n",
    "$$LCL = \\frac{s_{1}^2}{s_{2}^2} \\frac{1}{F_{\\alpha/2,\\nu_{1},\\nu_{2}}}$$\n",
    "$$UCL = \\frac{s_{1}^2}{s_{2}^2} F_{\\alpha/2,\\nu_{1},\\nu_{2}}$$\n",
    "\n",
    "## 13.2 Inference about the Difference between two Means: Matched Pairs Experiment\n",
    "\n",
    "$\\mu_{D}$ is the **mean of the population of differences**.\n",
    "\n",
    "Test Statistic for $\\mu_{D}$\n",
    "\n",
    "$$t = \\frac{\\bar x_{D} - \\mu_{D}}{s_{D}/\\sqrt{n_{D}}}$$\n",
    "\n",
    "which is Student t distributed with $\\nu = n_{D} - 1$ degrees of freedom, provided that the differences are normally distributed.\n",
    "\n",
    "Confidence Interval Estimator of $\\mu_{D}$\n",
    "\n",
    "$$\\bar x_{D} \\pm t_{\\alpha/2}\\frac{s_{D}}{\\sqrt{n_{D}}}$$\n",
    "\n",
    "## 13.3 Inference about the Difference between two Population Proportions\n",
    "\n",
    "The statistic $\\hat p_{1} − \\hat p_{2}$ is approximately normally distributed provided that the sample sizes are large enough so that $n_{1}p_{1}$, $n_{1}(1-p_{1})$, $n_{2}p_{2}$, and $n_{2}(1-p_{2})$ are all greater than or equal to 5.\n",
    "\n",
    "$$E(\\hat p_{1} − \\hat p_{2}) = p_{1} − p_{2}$$\n",
    "\n",
    "$$V(\\hat p_{1} − \\hat p_{2}) = \\frac{p_{1}(1-p_{1})}{n_{1}} + \\frac{p_{2}(1-p_{2})}{n_{2}}$$\n",
    "\n",
    "$$Z = \\frac{(\\hat p_{1} − \\hat p_{2}) - (p_{1} − p_{2})}{\\sqrt{\\frac{p_{1}(1-p_{1})}{n_{1}} + \\frac{p_{2}(1-p_{2})}{n_{2}}}}$$\n",
    "\n",
    "$$\\hat p_{1} = \\frac{x_{1}}{n_{1}}$$\n",
    "$$\\hat p_{2} = \\frac{x_{2}}{n_{2}}$$\n",
    "\n",
    "### 13.3.1 Test Statistic for $p_{1} − p_{2}$: Case 1\n",
    "\n",
    "$H_{0}$: $p_{1} − p_{2} = 0$\n",
    "\n",
    "$$z = \\frac{\\hat p_{1} − \\hat p_{2}}{\\sqrt{\\hat p(1-\\hat p)(\\frac{1}{n_{1}} + \\frac{1}{n_{2}})}}$$\n",
    "\n",
    "$$\\hat p = \\frac{x_{1} + x_{2}}{n_{1} + n_{2}}$$\n",
    "\n",
    "### 13.3.2 Test Statistic for $p_{1} − p_{2}$: Case 2\n",
    "\n",
    "$H_{0}$: $p_{1} − p_{2} = D, D\\ne0$\n",
    "\n",
    "$$z = \\frac{(\\hat p_{1} − \\hat p_{2}) - D}{\\sqrt{\\frac{\\hat p_{1}(1-\\hat p_{1})}{n_{1}} + \\frac{\\hat p_{2}(1-\\hat p_{2})}{n_{2}}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Analysis of Variance\n",
    "\n",
    "## 14.1 One-way Analysis of Variance\n",
    "\n",
    "The analysis of variance is a procedure that tests to determine whether differences exist between two or more population means. **one-way analysis of variance** is the procedure to apply when the samples are independently drawn.\n",
    "\n",
    "$H_{0}$: $\\mu_{1} = \\mu_{2} = \\cdots = \\mu_{k}$        \n",
    "$H_{1}$: at least two means differ        \n",
    "\n",
    "The statistic that measures the proximity of the sample means to each other is called the **between-treatments variation**; it is denoted **SST**, which stands for **sum of squares for treatments**.  \n",
    "\n",
    "$$SST = \\sum_{j=1}^k n_{j}(\\bar x_{j} -  \\bar{\\bar x})^2$$\n",
    "\n",
    "$$\\bar{\\bar x} =\\frac{\\sum_{j=1}^k \\sum_{i=1}^{n_{j}} x_{ij}}{n}$$\n",
    "\n",
    "$$n = n_{1} + n_{2} + \\cdots + n_{k}$$\n",
    "\n",
    "$$\\bar x_{j} = \\frac{\\sum_{i=1}^{n_{j}}x_{ij}}{n_{j}}$$\n",
    "\n",
    "how much variation exists in the percentage of assets, which is measured by the **within-treatments variation**, which is denoted by **SSE** (**sum of squares for error**). The within-treatments variation provides a measure of the amount of variation in the response variable that *is not caused by the treatments*.  \n",
    "\n",
    "$$SSE = \\sum_{j=1}^k \\sum_{i=1}^{n_{j}}(x_{ij} - \\bar x_{j})^2$$\n",
    "\n",
    "$$SSE = (n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2 + \\cdots + (n_{k}-1)s_{k}^2$$\n",
    "\n",
    "The mean square for treatments is computed by dividing SST by the number of treatments minus 1.\n",
    "\n",
    "$$MST = \\frac{SST}{k-1}$$\n",
    "\n",
    "The mean square for error is determined by dividing SSE by the total sample size (labeled n) minus the number of treatments.\n",
    "\n",
    "$$MSE = \\frac{SSE}{n-k}$$\n",
    "\n",
    "Finally, the test statistic is defined as the ratio of the two mean squares.\n",
    "\n",
    "$$F = \\frac{MST}{MSE}$$\n",
    "\n",
    "The test statistic is F-distributed with k − 1 and n − k degrees of freedom, provided that the response variable is normally distributed. we reject the null hypothesis only if\n",
    "\n",
    "$$F > F_{\\alpha, k-1, n-k}$$\n",
    "\n",
    "**total variation** of all the data is denoted **SS(Total)**\n",
    "\n",
    "$$SS(Total) = SST + SSE = \\sum_{j=1}^k \\sum_{i=1}^{n_{j}}(x_{ij} - \\bar{\\bar x})^2$$\n",
    "\n",
    "ANOVA Table for the One-Way Analysis of Variance:   \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES        | F-STATISTIC\n",
    "--------------------|--------------------|-----------------|---------------------|------------\n",
    "Treatments          |   k − 1            |     SST         | MST = SST/ (k − 1)  | F = MST/MSE\n",
    "Error               |   n − k            |     SSE         | MSE = SSE/ (n − k)  |\n",
    "Total               |   n − 1            |     SS(Total)   |                     |\n",
    "\n",
    "Example: a financial analyst randomly sampled 366 American households and asked each to report the age category of the head of the household and the proportion of its financial assets that are invested in the stock market. The age categories are\n",
    "Young (less than 35), Early middle age (35 to 49), Late middle age (50 to 65), Senior (older than 65). The analyst was particularly interested in determining whether the ownership of stocks varied by age.    \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES        | F-STATISTIC | P\n",
    "--------------------|--------------------|-----------------|---------------------|-------------|-------\n",
    "Treatments          |         3          |     3741.4      |  1247.12            | 2.79        | 0.0405\n",
    "Error               |         362        |     161871.0    |  447.16             |             |\n",
    "Total               |         365        |     165612.4    |                     |             |\n",
    "\n",
    "Interpret: The value of the test statistic is F = 2.79, and its p-value is .0405, which means there is evidence to infer that the percentage of total assets invested in stocks are different in at least two of the age categories.\n",
    "\n",
    "### 14.1.1 Can We Use the t-Test of the Difference between Two Means Instead of the Analysis of Variance?\n",
    "\n",
    "There are two reasons why we don’t use multiple t-tests instead of one F-test. First, we would have to perform many more calculations. Second, and more important, conducting multiple tests increases the probability of making Type I errors.\n",
    "\n",
    "### 14.1.2 Can We Use the Analysis of Variance Instead of the t-Test of $\\mu_{1} − \\mu_{2}$?\n",
    "\n",
    "If we want to determine whether $\\mu_{1}$ is greater than $\\mu_{2}$ (or vice versa), we cannot use the analysis of variance because this technique allows us to test for a difference only. Thus, if we want to test to determine whether one population mean exceeds the other, we must use the t-test of $\\mu_{1} − \\mu_{2}$ (with $\\sigma_{1}^2=\\sigma_{2}^2$). Moreover, the analysis of variance requires that the population variances are equal. If they are not, we must use the unequal variances test statistic.\n",
    "\n",
    "## 14.2 Multiple Comparisions\n",
    "\n",
    "Bonferroni adjustment:\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_{E}}{n}$$\n",
    "\n",
    "$\\alpha_{E}$, denotes the true probability of making at least one Type I error, is called the **experimentwise Type I error rate**. n is the number of pairwise comparisons.\n",
    "\n",
    "## 14.3 Analysis of Variance Experimental Designs\n",
    "\n",
    "### 14.3.1 Single-Factor and Multifactor Experimental Designs\n",
    "\n",
    "A **single-factor** analysis of variance addresses the problem of comparing two or more populations defined on the basis of only one factor. A **multifactor** experiment is one in which two or more factors define the treatments.       \n",
    "\n",
    "The example in `14.1` is a single-factor design because we had one treatment: age of the head of the household. Suppose that we can also look at the gender of the household head in another study. We would then develop a two-factor analysis of variance in which the first factor, age, has four levels, and the second factor, gender, has two levels.\n",
    "\n",
    "### 14.3.2 Independent Samples and Blocks\n",
    "\n",
    "When the problem objective is to compare more than two populations, the experimental design that is the counterpart of the matched pairs experiment is called the **randomized block design**. The term *block* refers to a matched group of observations from each population. The randomized block experiment is also called the **two-way analysis of variance**.    \n",
    "\n",
    "We can determine whether sleeping pills are effective by giving three brands of pills to the same group of people to measure the effects. Such experiments are called **repeated measures** designs.     \n",
    "\n",
    "The data are analyzed in the same way for both designs.       \n",
    "\n",
    "### 14.3.3 Fixed and Random Effects\n",
    "\n",
    "If our analysis includes all possible levels of a factor, the technique is called a **fixed effects analysis of variance**. If the levels included in the study represent a random sample of all the levels that exist, the technique is called a **random-effects analysis of variance**.\n",
    "\n",
    "## 14.4 Randomized Block (Two-Way) Analysis of Variance\n",
    "\n",
    "The purpose of designing a randomized block experiment is to reduce the within-treatments variation to more easily detect differences between the treatment means. In the one-way analysis of variance, we partitioned the total variation into the between-treatments and the within-treatments variation; that is,\n",
    "\n",
    "$$SS(Total) = SST + SSE$$\n",
    "\n",
    "In the randomized block design of the analysis of variance, we partition the total variation into three sources of variation:\n",
    "\n",
    "$$SS(Total) = SST + SSB + SSE$$\n",
    "\n",
    "where SSB, the **sum of squares for blocks**, measures the variation between the blocks.   \n",
    "\n",
    "BLOCK          |    1            |     2           |   ...    |    k            | Block Mean\n",
    "---------------|-----------------|-----------------|----------|-----------------|---------------\n",
    "1              | $x_{11}$        | $x_{12}$        |   ...    | $x_{1k}$        | $\\bar x[B]_{1}$\n",
    "2              | $x_{21}$        | $x_{22}$        |   ...    | $x_{2k}$        | $\\bar x[B]_{2}$\n",
    "$\\vdots$       | $\\vdots$        | $\\vdots$        |          | $\\vdots$        | $\\vdots$\n",
    "b              | $x_{b1}$        | $x_{b2}$        |   ...    | $x_{bk}$        | $\\bar x[B]_{b}$\n",
    "Treatment Mean | $\\bar x[T]_{1}$ | $\\bar x[T]_{2}$ |   ...    | $\\bar x[T]_{k}$ | \n",
    "\n",
    "Sums of Squares in the Randomized Block Experiment:\n",
    "\n",
    "$$SS(Total) = \\sum_{j=1}^k \\sum_{i=1}^b (x_{ij} - \\bar{\\bar x})^2$$\n",
    "$$SST = \\sum_{j=1}^k b(\\bar x[T]_{j} -  \\bar{\\bar x})^2$$\n",
    "$$SSB = \\sum_{i=1}^b k(\\bar x[B]_{i} - \\bar{\\bar x})^2$$\n",
    "$$SSE = \\sum_{j=1}^k \\sum_{i=1}^b (x_{ij} - \\bar x[T]_{j} - \\bar x[B]_{i} + \\bar{\\bar x})^2$$\n",
    "\n",
    "Mean Squares for the Randomized Block Experiment:\n",
    "\n",
    "$$MST = \\frac{SST}{k-1}$$\n",
    "$$MSB = \\frac{SSB}{b-1}$$\n",
    "$$MSE = \\frac{SSE}{n-k-b-1}$$\n",
    "\n",
    "Test Statistic for the Randomized Block Experiment\n",
    "\n",
    "$$F = \\frac{MST}{MSE}$$\n",
    "\n",
    "which is F-distributed with ν1 = k − 1 and ν2 = n − k − b + 1 degrees of freedom.        \n",
    "\n",
    "ANOVA Table for the Randomized Block Analysis of Variance   \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES                 | F-STATISTIC\n",
    "--------------------|--------------------|-----------------|------------------------------|------------\n",
    "Treatments          |   k − 1            |     SST         | MST = SST / (k − 1)          | F = MST/MSE\n",
    "Blocks              |   b - 1            |     SSB         | MSB = SSB / (b - 1)          | F = MSB/MSE\n",
    "Error               |   n − k - b + 1    |     SSE         | MSE = SSE / (n − k - b + 1)  |\n",
    "Total               |   n − 1            |     SS(Total)   |                              |\n",
    "\n",
    "Example: A company selected 25 groups of four men, each of whom had cholesterol levels in excess of 280. In each group, the men were matched according to age and weight. Four drugs were administered over a 2-month period, and the reduction in cholesterol was recorded. Do these results allow the company to conclude that differences exist between the four drugs?   \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES | F-STATISTIC | P   \n",
    "--------------------|--------------------|-----------------|--------------|-------------|------\n",
    "Drug                |         3          |     196.0       |  65.3        |  4.12       | 0.009\n",
    "Group               |         24         |     3848.7      |  160.4       |  10.11      | 0.000\n",
    "Error               |         72         |     1142.6      |  15.9        |             |\n",
    "Total               |         99         |     5187.2      |              |             |\n",
    "\n",
    "Interpret: we conclude that there is sufficient evidence to infer that at least two of the drugs differ.\n",
    "\n",
    "## 14.5 Two-Factor Analysis of Variance\n",
    "\n",
    "The general term for the experiment features two factors is **factorial experiment**. In factorial experiments, we can examine the effect on the response variable of two or more factors. We will present the technique for fixed effects only. That means we will address problems where all the levels of the factors are included in the experiment.     \n",
    "\n",
    "Example: As part of a study on job tenure, a survey was conducted in which Americans aged between 37 and 45 were asked how many jobs they have held in their lifetimes. Also recorded were gender and educational attainment. The categories are E1, E2, E3 and E4. Can we infer that differences exist between genders and educational levels?    \n",
    "\n",
    "$H_{0}$: $\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5} = \\mu_{6} = \\mu_{7} = \\mu_{8}$           \n",
    "$H_{1}$: At least two means differ\n",
    "\n",
    "Summary:    \n",
    "\n",
    "Groups      | Count | Sum | Average | Variance\n",
    "------------|-------|-----|---------|---------\n",
    "Male E1     | 10    | 126 | 12.60   | 8.27\n",
    "Male E2     | 10    | 110 | 11.00   | 8.67\n",
    "Male E3     | 10    | 106 | 10.60   | 11.60\n",
    "Male E4     | 10    | 90  | 9.00    | 5.33\n",
    "Female E1   | 10    | 115 | 11.50   | 8.28\n",
    "Female E2   | 10    | 112 | 11.20   | 9.73\n",
    "Female E3   | 10    | 94  | 9.40    | 16.49\n",
    "Female E4   | 10    | 81  | 8.10    | 12.32\n",
    "\n",
    "one-way Anova:    \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES | F-STATISTIC | P   \n",
    "--------------------|--------------------|-----------------|--------------|-------------|------\n",
    "Between Groups      |         7          |     153.35      |  21.91       |  2.17       | 0.0467\n",
    "Within Groups       |         72         |     726.20      |  10.09       |             |\n",
    "Total               |         79         |     879.55      |              |             |\n",
    "\n",
    "Interpret: The value of the test statistic is F = 2.17 with a p-value of .0467. We conclude that there are differences in the number of jobs between the eight treatments.     \n",
    "\n",
    "This statistical result raises more questions—namely, can we conclude that the differences in the mean number of jobs are caused by differences between males and females? Or are they caused by differences between educational levels? Or, perhaps, are there combinations, called interactions, of gender and education that result in especially high or low numbers?      \n",
    "\n",
    "A **complete factorial experiment** is an experiment in which the data for all possible combinations of the levels of the factors are gathered. That means that in the above example we measured the number of jobs for all eight combinations. This experiment is called a complete 2 × 4 factorial experiment. In general, we will refer to one of the factors as factor A (arbitrarily chosen). The number of levels of this factor will be denoted by a. The other factor is called factor B, and its number of levels is denoted by b. The number of observations for each combination is called a replicate. The number of replicates is denoted by r. We address only problems in which the number of replicates is the same for each treatment. Such a design is called **balanced**.  \n",
    "\n",
    "![](fig/stat-two-way-anova.png)\n",
    "\n",
    "$x_{ijk}$ = $k$th observation in the $ij$th treatment  \n",
    "$\\bar x[AB]_{ij}=$ mean of the treatment when the factor A level is i and the factor B level is j   \n",
    "$\\bar x[A]_{i}=$ Mean of the observations when the factor A level is i     \n",
    "$\\bar x[B]_{j}=$ Mean of the observations when the factor B level is j      \n",
    "$\\bar{\\bar x}=$ Mean of all the observations      \n",
    "a = Number of factor A levels       \n",
    "b = Number of factor B levels        \n",
    "r = Number of replicates       \n",
    "\n",
    "$$SS(Total) = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^r (x_{ijk} - \\bar{\\bar x})^2$$\n",
    "$$SS(A) = rb \\sum_{i=1}^a (\\bar x[A]_{i} -  \\bar{\\bar x})^2$$\n",
    "$$SS(B) = ra \\sum_{j=1}^b (\\bar x[B]_{j} -  \\bar{\\bar x})^2$$\n",
    "$$SS(AB) = r \\sum_{i=1}^a \\sum_{j=1}^b (\\bar x[AB]_{ij} - \\bar x[A]_{i} -  \\bar x[B]_{j} +  \\bar{\\bar x})^2$$\n",
    "$$SSE = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{k=1}^r  (x_{ijk} - \\bar x[AB]_{ij})^2$$\n",
    "\n",
    "$\\nu_{SS(A)} = a -1$      \n",
    "$\\nu_{SS(B)} = b -1$         \n",
    "$\\nu_{SS(AB)} = (a -1)(b-1)$          \n",
    "$\\nu_{SSE} = n - ab$          \n",
    "\n",
    "**F-Tests Conducted in Two-Factor Analysis of Variance**    \n",
    "*Test for Differences between the Levels of Factor A*    \n",
    "$H_{0}$: The means of the a levels of factor A are equal                \n",
    "$H_{1}$: At least two means differ       \n",
    "*Test for Differences between the Levels of Factor B*    \n",
    "$H_{0}$: The means of the a levels of factor B are equal                \n",
    "$H_{1}$: At least two means differ    \n",
    "*Test for Interaction between Factors A and B*    \n",
    "$H_{0}$: Factors A and B do not interact to affect the mean responses                     \n",
    "$H_{1}$: Factors A and B do interact to affect the mean responses   \n",
    "\n",
    "**Required Conditions**          \n",
    "* The distribution of the response is normally distributed.        \n",
    "* The variance for each treatment is identical.          \n",
    "* The samples are independent.            \n",
    "\n",
    "ANOVA Table for the Two-Factor Experiment:            \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES | F-STATISTIC \n",
    "--------------------|--------------------|-----------------|--------------|-------------\n",
    "Factor A            |         a-1        |     SS(A)       |  MS(A)       |  MS(A)/MSE       \n",
    "Factor B            |         b-1        |     SS(B)       |  MS(B)       |  MS(B)/MSE        \n",
    "Interaction         |         (a-1)(b-1) |     SS(AB)      |  MS(AB)      |  MS(AB)/MSE        \n",
    "Error               |         n - ab     |     SSE         |  MSE         |             \n",
    "Total               |         n -1       |     SS(Total)   |              |             \n",
    "\n",
    "Two-way ANOVA: Jobs versus Gender, Education    \n",
    "\n",
    "SOURCE OF VARIATION | DEGREES OF FREEDOM | SUMS OF SQUARES | MEAN SQUARES | F-STATISTIC | P\n",
    "--------------------|--------------------|-----------------|--------------|-------------|---------\n",
    "Gender              |         1          |     11.25       |  11.25       |  1.12       | 0.294       \n",
    "Education           |         3          |     135.85      |  45.28       |  4.49       | 0.006        \n",
    "Interaction         |         3          |     6.25        |  2.08        |  0.21       | 0.892        \n",
    "Error               |         72         |     726.20      |  10.09       |             |            \n",
    "Total               |         79         |     879.55      |              |             |  \n",
    "\n",
    "Interpret: There is no evidence at the 5% significance level to infer that differences in the number of jobs exist between men and women. There is sufficient evidence at the 5% significance level to infer that differences in the number of jobs exist between educational levels. There is not enough evidence to conclude that there is an interaction between gender and education.        \n",
    "\n",
    "**Order of Testing in the Two-Factor Analysis of Variance:** Test for interaction first. If there is enough evidence to infer that there is interaction, do not conduct the other tests. If there is not enough evidence to conclude that there is interaction, proceed to conduct the F-tests for factors A and B.        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Chi-Squared Tests\n",
    "\n",
    "A **multinomial experiment** is one that possesses the following properties:\n",
    "\n",
    "* The experiment consists of a fixed number n of trials.\n",
    "* The outcome of each trial can be classified into one of k categories, called cells. $f_{i}$ is the observed frequency of outcomes falling into cell i, $f_{1} + f{2} + \\cdots + f_{k} = n$\n",
    "* The probability $p_{i}$ is that the outcome will fall into cell i remains constant for each trial. Moreover, $p_{1} + p{2} + \\cdots + p_{k} = 1$\n",
    "* Each trial of the experiment is independent of the other trials.\n",
    "\n",
    "## 15.1 Chi-Squared Goodness-Of-Fit Test\n",
    "\n",
    "Chi-Squared Goodness-of-Fit Test Statistic          \n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^k\\frac{(f_{i}-e_{i})^2}{e_{i}}$$\n",
    "\n",
    "**rule of five**: which states that the sample size must be large enough so that the expected value for each cell must be 5 or more.      \n",
    "\n",
    "Example: Current share of the market of Company A, Company B and others is 45%, 40% and 15%, respectively. Company A has recently conducted aggressive advertising campaigns to maintain and possibly increase its share of the market. A random sample of 200 customers, 102 indicated a preference for company A’s product, 82 preferred company B’s product, and the remaining 16 preferred the products of others. Can the analyst infer at the 5% significance level that customer preferences have changed from their levels before the advertising campaigns were launched?        \n",
    "\n",
    "$H_{0}$: $p_{1}=0.45$, $p_{2}=0.4$, $p_{3}=0.15$         \n",
    "$H_{1}$: At least one p_{i} is not equal to its specified value        \n",
    "\n",
    "expect the number of customers:      \n",
    "\n",
    "$e_{1}=200 \\times 0.45 = 90$       \n",
    "$e_{2}=200 \\times 0.40 = 80$        \n",
    "$e_{3}=200 \\times 0.15 = 30$   \n",
    "\n",
    "Observed number of customers:     \n",
    "\n",
    "$f_{1}=102$               \n",
    "$f_{2}=82$                  \n",
    "$f_{3}=16$         \n",
    "\n",
    "$\\chi^2=8.18$, $\\chi_{\\alpha, k-1}^2=5.99$, we reject the null hypothesis. The p-value of the test is\n",
    "\n",
    "$$p-value = P(\\chi^2 \\gt 8.18)=0.0167$$\n",
    "\n",
    "## 15.2 Chi-Squared Test of a Contingency Table\n",
    "\n",
    "The chi-squared test of a contingency table is used to determine whether there is enough evidence to infer that two nominal variables are **related** and to infer that differences exist between two or more populations of nominal variables.\n",
    "\n",
    "The test statistic is the same as the one used to test proportions in the goodness-of-fit test     \n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^k\\frac{(f_{i}-e_{i})^2}{e_{i}}$$\n",
    "\n",
    "The number of degrees of freedom for a contingency table with r rows and c columns is $\\nu = (r − 1)(c − 1)$.     \n",
    "\n",
    "Expected Frequencies for a Contingency Table:     \n",
    "\n",
    "$$e_{ij} = np(AB) = np(A)p(B)= n\\frac{nr_{i}}{n}\\frac{nc_{j}}{n} = \\frac{nr_{i} \\times nc_{j}}{n}$$\n",
    "\n",
    "* n is sample size      \n",
    "* $nr_{i}$ is row i total    \n",
    "* $nc_{j}$ is column j total   \n",
    "\n",
    "Example: a random sample of last year’s MBA students and recorded the undergraduate degree and the major selected in the graduate program. The undergraduate degrees were BA, BEng, BBA, and several others. There are three possible majors for the MBA students: accounting, finance, and marketing. Can the statistician conclude that the undergraduate degree affects the choice of major?\n",
    "\n",
    "Undergraduate Degree |  Accounting  |  Finance  |  Marketing  |  Total\n",
    "---------------------|--------------|-----------|-------------|--------\n",
    "BA                   |     31       |     13    |     16      |  60\n",
    "BEng                 |     8        |     16    |     7       |  31\n",
    "BBA                  |     12       |     10    |     17      |  39\n",
    "Other                |     10       |     5     |     7       |  22\n",
    "Total                |     61       |     44    |     47      |  152\n",
    "\n",
    "$\\chi^2 = 14.7$, $\\chi_{\\alpha, \\nu}^2 = \\chi_{0.05, 6}^2 = 12.6$, we reject the null hypothesis and conclude that there is evidence of a relationship between undergraduate degree and MBA major. The p-value of the test statistic is\n",
    "\n",
    "$$p-value = P(\\chi^2 \\gt 14.7)=0.0227$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 16. Simple linear regression and correlation\n",
    "\n",
    "In this chapter, we describe only the straight-line model with **one independent variable**. This model is called the **first-order linear model**—sometimes called the **simple linear regression model**.\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x + \\epsilon$$\n",
    "\n",
    "The straight line that we wish to use to estimate $\\beta_{0}$ and $\\beta_{1}$ is the “best” straight line—best in the sense that it comes closest to the sample data points. This best straight line, called **the least squares line**, is derived from calculus and is represented by the following equation:\n",
    "\n",
    "$$\\hat y = b_{0} + b_{1}x$$\n",
    "\n",
    "Least Squares Line Coefficients:   \n",
    "\n",
    "$$b_{1} = \\frac{s_{xy}}{s_{x}^2} $$\n",
    "\n",
    "$$b_{0} = \\bar y - b_{1}\\bar x$$\n",
    "\n",
    "where   \n",
    "\n",
    "$$s_{xy} = \\frac{\\sum_{i=1}^n{(x_{i} -\\bar x)(y_{i} - \\bar y)}}{n-1}$$\n",
    "\n",
    "$$s_{x}^2 = \\frac{\\sum_{i=1}^n{(x_{i} -\\bar x)^2}}{n-1}$$\n",
    "\n",
    "The deviations between the actual data points and the line are called **residuals**, denoted $e_{i}$; that is,  \n",
    "\n",
    "$$e_{i} = y_{i} - \\hat y$$ \n",
    "\n",
    "Required Conditions for the Error Variable:    \n",
    "\n",
    "- The probability distribution of ε is normal.     \n",
    "- The mean of the distribution is 0; that is, E(ε) = 0.     \n",
    "- The standard deviation of ε ($\\sigma_{e}$) is a constant regardless of the value of x.    \n",
    "- The value of ε associated with any particular value of y is independent of ε associated with any other value of y.    \n",
    "\n",
    "The residuals are observations of the error variable. Consequently, the **minimized sum of squared deviations** is called the **sum of squares for error, denoted SSE**.  \n",
    "\n",
    "$$SSE = \\sum_{i=1}^n(y_{i} - \\hat y_{i})^2$$   \n",
    "\n",
    "The unbiased estimator of the **variance of the error variable** $\\sigma_{e}^2$ is    \n",
    "\n",
    "$$s_{e}^2 = \\frac{SSE}{n-2}$$\n",
    "\n",
    "The square root of $s_{e}^2$ is called the **standard error of estimate**.\n",
    "\n",
    "$$s_{e} = \\sqrt{\\frac{SSE}{n-2}}$$\n",
    "\n",
    "The smallest value that $s_{e}$ can assume is 0, which occurs when SSE = 0, that is, when all the points fall on the regression line. Thus, when $s_{e}$ is small, the fit is excellent, and the linear model is likely to be an effective analytical and forecasting tool. If $s_{e}$ is large, the model is a poor one, and the statistics practitioner should improve it or discard it.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 Testing the Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
